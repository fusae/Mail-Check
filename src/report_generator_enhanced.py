#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
èˆ†æƒ…æŠ¥å‘Šç”Ÿæˆå™¨ - å¢å¼ºç‰ˆ
å‚è€ƒç¤ºä¾‹æŠ¥å‘Šé£æ ¼ï¼Œç”Ÿæˆæ›´åŠ è¯¦ç»†å’Œä¸“ä¸šçš„èˆ†æƒ…åˆ†ææŠ¥å‘Š
Enhanced Sentiment Report Generator
"""

import pandas as pd
import numpy as np
from datetime import datetime, timedelta
from typing import Dict, List, Any
from collections import Counter
import re
import json

try:
    import jieba
    JIEBA_AVAILABLE = True
except ImportError:
    JIEBA_AVAILABLE = False

try:
    from docx import Document
    from docx.shared import Inches, Pt, RGBColor
    from docx.enum.text import WD_ALIGN_PARAGRAPH
    DOCX_AVAILABLE = True
except ImportError:
    DOCX_AVAILABLE = False


class EnhancedReportGenerator:
    """å¢å¼ºç‰ˆèˆ†æƒ…æŠ¥å‘Šç”Ÿæˆå™¨"""

    def __init__(self):
        self.platform_names = {
            'æŠ–éŸ³': 'æŠ–éŸ³',
            'douyin': 'æŠ–éŸ³',
            'æ–°æµªå¾®åš': 'å¾®åš',
            'å¾®åš': 'å¾®åš',
            'weibo': 'å¾®åš',
            'å¾®ä¿¡': 'å¾®ä¿¡',
            'wechat': 'å¾®ä¿¡',
            'æ–°é—»ç½‘ç«™': 'æ–°é—»ç½‘ç«™',
            'news': 'æ–°é—»ç½‘ç«™',
            'é»‘çŒ«æŠ•è¯‰': 'é»‘çŒ«æŠ•è¯‰',
            'ä»Šæ—¥å¤´æ¡': 'ä»Šæ—¥å¤´æ¡',
            'ç™¾åº¦è´´å§': 'ç™¾åº¦è´´å§',
        }

        # æƒ…æ„Ÿå…³é”®è¯åº“
        self.emotion_keywords = {
            'æ„¤æ€’': ['æ„¤æ€’', 'ç”Ÿæ°”', 'ç«å¤§', 'æ— è¯­', 'å‡­ä»€ä¹ˆ', 'å‡­ä»€ä¹ˆ', 'å¿æ— å¯å¿',
                    'å¤ªå·®äº†', 'åƒåœ¾', 'æ— è‰¯', 'é»‘å¿ƒ', 'éª—å­', 'ä¸è´Ÿè´£ä»»'],
            'æ‚²ä¼¤': ['éš¾è¿‡', 'å¿ƒç—›', 'æ‚²ä¼¤', 'ç—›è‹¦', 'ä¸å¹¸', 'å»ä¸–', 'æ­»äº¡', 'ç¦»å¼€',
                    'å¥½å¥½çš„ä¸€ä¸ªäºº', 'å†ä¹Ÿè§ä¸åˆ°', 'é—æ†¾', 'æƒ‹æƒœ'],
            'å¤±æœ›': ['å¤±æœ›', 'å¤±æœ›é€é¡¶', 'å¤ªå¤±æœ›äº†', 'ä¸å€¼', 'ä¸å€¼å¾—', 'ç™½è·‘ä¸€è¶Ÿ',
                    'æµªè´¹æ—¶é—´', 'ä¸æ¨è', 'å†ä¹Ÿä¸æ¥äº†'],
            'è´¨ç–‘': ['è´¨ç–‘', 'æ€€ç–‘', 'çœŸçš„å—', 'å¯ä¿¡å—', 'é è°±å—', 'æ˜¯ä¸æ˜¯',
                    'å‡­ä»€ä¹ˆè¯´', 'è¯æ®å‘¢', 'æœ‰è¯æ®å—', 'çœŸå‡'],
            'æ‹…å¿§': ['æ‹…å¿ƒ', 'æ‹…å¿§', 'å®³æ€•', 'ææƒ§', 'ä¸æ•¢', 'å®³æ€•å»',
                    'æœ‰é£é™©', 'ä¸å®‰å…¨', 'å¯æ€•'],
        }

        # é£é™©ç­‰çº§æ˜ å°„
        self.risk_level_map = {
            'high': 'ğŸ”´ æé«˜',
            'medium': 'ğŸŸ  é«˜',
            'low': 'ğŸŸ¡ ä¸­'
        }

    def normalize_platform(self, platform: str) -> str:
        """æ ‡å‡†åŒ–å¹³å°åç§°"""
        platform = platform.lower().strip()
        for key, value in self.platform_names.items():
            if key in platform:
                return value
        return platform

    def generate_report_data(
        self,
        df: pd.DataFrame,
        hospital_name: str,
        report_type: str = "special",
        report_period: str = None
    ) -> Dict[str, Any]:
        """
        ç”Ÿæˆå¢å¼ºç‰ˆæŠ¥å‘Šæ•°æ®

        å‚æ•°ï¼š
        - df: èˆ†æƒ…æ•°æ®DataFrame
        - hospital_name: åŒ»é™¢åç§°
        - report_type: æŠ¥å‘Šç±»å‹ï¼ˆspecial/quarterly/monthlyï¼‰
        - report_period: æŠ¥å‘Šå‘¨æœŸï¼ˆå¦‚"2026Q1"ï¼‰
        """
        # æ•°æ®é¢„å¤„ç†
        df = self._preprocess_data(df)

        # ç”Ÿæˆå„ä¸ªéƒ¨åˆ†çš„æ•°æ®
        report_data = {
            'hospital_name': hospital_name,
            'report_type': report_type,
            'report_period': report_period or self._auto_detect_period(df),
            'generated_time': datetime.now().strftime('%Yå¹´%mæœˆ%dæ—¥ %H:%M'),
            'report_date': datetime.now().strftime('%Yå¹´%mæœˆ%dæ—¥'),
            'report_date_range': self._get_report_date_range(df),
            'summary': self._generate_summary(df),
            'overview': self._generate_overview(df),
            'distribution': self._generate_distribution(df),
            'key_events': self._generate_key_events_enhanced(df),  # å¢å¼ºç‰ˆå…³é”®äº‹ä»¶
            'sentiment': self._generate_sentiment_enhanced(df),  # å¢å¼ºç‰ˆæƒ…æ„Ÿåˆ†æ
            'risk_assessment': self._generate_risk_assessment_enhanced(df),  # å¢å¼ºç‰ˆé£é™©è¯„ä¼°
            'recommendations': self._generate_recommendations_enhanced(df),  # å¢å¼ºç‰ˆå»ºè®®
            'impact_forecast': self._generate_impact_forecast(df),  # æ–°å¢ï¼šå½±å“é¢„æµ‹
            'response_templates': self._generate_response_templates(df),  # æ–°å¢ï¼šåº”å¯¹æ¨¡æ¿
            'appendix': self._generate_appendix_enhanced(df),  # å¢å¼ºç‰ˆé™„å½•
            'raw_dataframe': df
        }

        return report_data

    def _preprocess_data(self, df: pd.DataFrame) -> pd.DataFrame:
        """æ•°æ®é¢„å¤„ç†"""
        df = df.copy()

        # æ ‡å‡†åŒ–å¹³å°åç§°
        df['æ¥æº_æ ‡å‡†'] = df['æ¥æº'].apply(self.normalize_platform)

        # è§£ææ—¶é—´
        df['åˆ›å»ºæ—¶é—´_è§£æ'] = pd.to_datetime(df['åˆ›å»ºæ—¶é—´'], errors='coerce')

        # æå–æ—¥æœŸå’Œå°æ—¶
        df['æ—¥æœŸ'] = df['åˆ›å»ºæ—¶é—´_è§£æ'].dt.date
        df['å°æ—¶'] = df['åˆ›å»ºæ—¶é—´_è§£æ'].dt.hour
        df['æ—¥æœŸ_å­—ç¬¦ä¸²'] = df['åˆ›å»ºæ—¶é—´_è§£æ'].dt.strftime('%Y-%m-%d')
        df['æ—¶é—´_å­—ç¬¦ä¸²'] = df['åˆ›å»ºæ—¶é—´_è§£æ'].dt.strftime('%Y-%m-%d %H:%M')

        # è®¡ç®—é£é™©åˆ†
        df['é£é™©åˆ†_æ•°å€¼'] = pd.to_numeric(df['é£é™©åˆ†'], errors='coerce').fillna(0)

        return df

    def _auto_detect_period(self, df: pd.DataFrame) -> str:
        """è‡ªåŠ¨æ£€æµ‹æŠ¥å‘Šå‘¨æœŸ"""
        if len(df) == 0:
            return datetime.now().strftime('%Yå¹´%mæœˆ')

        dates = pd.to_datetime(df['åˆ›å»ºæ—¶é—´'], errors='coerce').dropna()
        if len(dates) == 0:
            return datetime.now().strftime('%Yå¹´%mæœˆ')

        min_date = dates.min()
        max_date = dates.max()

        if min_date.month == max_date.month:
            return min_date.strftime('%Yå¹´%mæœˆ')
        elif min_date.year == max_date.year:
            quarter = (max_date.month - 1) // 3 + 1
            return f"{max_date.year}Q{quarter}"
        else:
            return f"{min_date.strftime('%Yå¹´%mæœˆ')}-{max_date.strftime('%Yå¹´%mæœˆ')}"

    def _get_report_date_range(self, df: pd.DataFrame) -> str:
        """è·å–æŠ¥å‘Šæ—¥æœŸèŒƒå›´"""
        if len(df) == 0:
            return "æ— æ•°æ®"

        dates = pd.to_datetime(df['åˆ›å»ºæ—¶é—´'], errors='coerce').dropna()
        if len(dates) == 0:
            return "æ— æ•°æ®"

        min_date = dates.min()
        max_date = dates.max()

        return f"{min_date.strftime('%Yå¹´%mæœˆ%dæ—¥')}-{max_date.strftime('%Yå¹´%mæœˆ%dæ—¥')}"

    def _generate_summary(self, df: pd.DataFrame) -> Dict[str, Any]:
        """ç”ŸæˆæŠ¥å‘Šæ‘˜è¦ï¼ˆå¢å¼ºç‰ˆï¼‰"""
        total = len(df)
        high_risk = len(df[df['ä¸¥é‡ç¨‹åº¦'] == 'high'])
        medium_risk = len(df[df['ä¸¥é‡ç¨‹åº¦'] == 'medium'])
        active = len(df[df['çŠ¶æ€'] == 'active'])
        avg_risk = df['é£é™©åˆ†_æ•°å€¼'].mean()

        # ä¼°ç®—å½±å“äººæ•°
        estimated_reach = self._estimate_reach(df)

        # ä¼ æ’­å³°å€¼æ—¶é—´
        peak_time = self._find_peak_time(df)

        # è¶‹åŠ¿åˆ†æ
        trend = self._analyze_trend(df)

        # å±é™©çº§åˆ«åˆ¤æ–­
        danger_level = self._assess_danger_level(df)

        return {
            'total_events': total,
            'high_risk_events': high_risk,
            'medium_risk_events': medium_risk,
            'active_events': active,
            'inactive_events': total - active,
            'average_risk_score': round(avg_risk, 1),
            'estimated_reach': estimated_reach,
            'peak_time': peak_time,
            'trend': trend,
            'danger_level': danger_level,
            'platforms': df['æ¥æº_æ ‡å‡†'].nunique(),
            'departments': df.get('ç§‘å®¤', pd.Series()).nunique()
        }

    def _assess_danger_level(self, df: pd.DataFrame) -> str:
        """è¯„ä¼°å±é™©çº§åˆ«"""
        if len(df) == 0:
            return "ğŸŸ¢ æ— é£é™©"

        high_risk = len(df[df['ä¸¥é‡ç¨‹åº¦'] == 'high'])
        avg_risk = df['é£é™©åˆ†_æ•°å€¼'].mean()

        if avg_risk >= 90 or high_risk >= 5:
            return "ğŸ”´ æé«˜å±é™©çº§åˆ«ï¼"
        elif avg_risk >= 70 or high_risk >= 3:
            return "ğŸŸ  é«˜å±é™©çº§åˆ«"
        elif avg_risk >= 50 or high_risk >= 1:
            return "ğŸŸ¡ ä¸­å±é™©çº§åˆ«"
        else:
            return "ğŸŸ¢ ä½å±é™©çº§åˆ«"

    def _estimate_reach(self, df: pd.DataFrame) -> str:
        """ä¼°ç®—å½±å“äººæ•°ï¼ˆå¢å¼ºç‰ˆï¼‰"""
        if len(df) == 0:
            return "0"

        total = 0
        for _, row in df.iterrows():
            platform = row.get('æ¥æº_æ ‡å‡†', '')
            severity = row.get('ä¸¥é‡ç¨‹åº¦', 'low')

            # æ ¹æ®å¹³å°å’Œä¸¥é‡ç¨‹åº¦ä¼°ç®—
            base_reach = 1000  # é»˜è®¤1000äºº

            if 'æŠ–éŸ³' in platform:
                base_reach = 100000  # æŠ–éŸ³10ä¸‡
            elif 'å¾®åš' in platform:
                base_reach = 50000  # å¾®åš5ä¸‡
            elif 'å¾®ä¿¡' in platform:
                base_reach = 10000  # å¾®ä¿¡1ä¸‡

            if severity == 'high':
                base_reach *= 10
            elif severity == 'medium':
                base_reach *= 3

            total += base_reach

        if total >= 10000000:
            return f"{total // 10000000}åƒä¸‡+"
        elif total >= 10000:
            return f"{total // 10000}ä¸‡+"
        elif total >= 1000:
            return f"{total // 1000}åƒ+"
        else:
            return str(total)

    def _find_peak_time(self, df: pd.DataFrame) -> str:
        """æ‰¾åˆ°ä¼ æ’­å³°å€¼æ—¶é—´"""
        if len(df) == 0:
            return "æœªçŸ¥"

        daily_counts = df.groupby('æ—¥æœŸ').size()
        if len(daily_counts) == 0:
            return "æœªçŸ¥"

        peak_date = daily_counts.idxmax()
        return peak_date.strftime('%Yå¹´%mæœˆ%dæ—¥')

    def _analyze_trend(self, df: pd.DataFrame) -> str:
        """åˆ†æè¶‹åŠ¿ï¼ˆå¢å¼ºç‰ˆï¼‰"""
        if len(df) < 2:
            return "æ•°æ®ä¸è¶³"

        df_sorted = df.sort_values('åˆ›å»ºæ—¶é—´_è§£æ')

        first_half = df_sorted[:len(df_sorted)//2]
        second_half = df_sorted[len(df_sorted)//2:]

        first_count = len(first_half)
        second_count = len(second_half)

        if second_count > first_count * 1.5:
            return "ğŸ“ˆ å¿«é€Ÿä¸Šå‡"
        elif second_count < first_count * 0.7:
            return "ğŸ“‰ ä¸‹é™"
        else:
            return "â¡ï¸ å¹³ç¨³"

    def _generate_overview(self, df: pd.DataFrame) -> Dict[str, Any]:
        """ç”Ÿæˆæ¦‚è¿°æ•°æ®"""
        total = len(df)

        # æŒ‰ä¸¥é‡ç¨‹åº¦ç»Ÿè®¡
        severity_counts = df['ä¸¥é‡ç¨‹åº¦'].value_counts()

        # æŒ‰çŠ¶æ€ç»Ÿè®¡
        status_counts = df['çŠ¶æ€'].value_counts()

        # æŒ‰å¹³å°ç»Ÿè®¡
        platform_counts = df['æ¥æº_æ ‡å‡†'].value_counts()

        return {
            'total': total,
            'severity_distribution': {
                'high': int(severity_counts.get('high', 0)),
                'medium': int(severity_counts.get('medium', 0)),
                'low': int(severity_counts.get('low', 0))
            },
            'status_distribution': status_counts.to_dict(),
            'platform_distribution': platform_counts.to_dict(),
            'average_risk_score': round(df['é£é™©åˆ†_æ•°å€¼'].mean(), 1),
            'max_risk_score': int(df['é£é™©åˆ†_æ•°å€¼'].max()),
            'min_risk_score': int(df['é£é™©åˆ†_æ•°å€¼'].min())
        }

    def _generate_distribution(self, df: pd.DataFrame) -> Dict[str, Any]:
        """ç”Ÿæˆåˆ†å¸ƒåˆ†æ"""
        return {
            'time_distribution': self._analyze_time_distribution_enhanced(df),
            'platform_distribution': self._analyze_platform_distribution_enhanced(df),
            'type_distribution': self._analyze_type_distribution_enhanced(df),
            'department_distribution': self._analyze_department_distribution_enhanced(df)
        }

    def _analyze_time_distribution_enhanced(self, df: pd.DataFrame) -> Dict[str, Any]:
        """æ—¶é—´åˆ†å¸ƒåˆ†æï¼ˆå¢å¼ºç‰ˆï¼‰"""
        if len(df) == 0:
            return {'timeline': [], 'pattern': 'æ— æ•°æ®', 'peak_hours': []}

        # æŒ‰æ—¥æœŸç»Ÿè®¡
        daily_counts = df.groupby('æ—¥æœŸ').size().sort_index()

        # æ„å»ºæ—¶é—´è½´
        timeline = []
        for date, count in daily_counts.items():
            date_str = date.strftime('%mæœˆ%dæ—¥')
            events = df[df['æ—¥æœŸ'] == date].sort_values('åˆ›å»ºæ—¶é—´_è§£æ')

            # è·å–è¯¥æ—¥çš„æ—¶é—´æ®µ
            time_slots = []
            for _, event in events.iterrows():
                hour = event.get('å°æ—¶', 0)
                time_slots.append(f"{hour:02d}:00")

            timeline.append({
                'date': date_str,
                'count': int(count),
                'time_slots': time_slots,
                'platforms': events['æ¥æº_æ ‡å‡†'].unique().tolist()
            })

        # æŒ‰å°æ—¶ç»Ÿè®¡
        hourly_counts = df.groupby('å°æ—¶').size()

        # æ‰¾å‡ºå³°å€¼æ—¶æ®µ
        peak_hours = sorted(hourly_counts.items(), key=lambda x: x[1], reverse=True)[:5]

        # æ£€æµ‹æ—¶é—´æ¨¡å¼
        time_pattern = self._detect_time_pattern(df)

        return {
            'timeline': timeline,
            'daily_counts': {str(k): int(v) for k, v in daily_counts.items()},
            'hourly_counts': {int(k): int(v) for k, v in hourly_counts.items()},
            'peak_hours': [{'hour': int(h), 'count': int(c)} for h, c in peak_hours],
            'time_pattern': time_pattern
        }

    def _detect_time_pattern(self, df: pd.DataFrame) -> str:
        """æ£€æµ‹æ—¶é—´æ¨¡å¼"""
        if len(df) == 0:
            return "æ— æ•°æ®"

        hour_counts = df.groupby('å°æ—¶').size()

        # åˆ¤æ–­æ˜¯å¦å¤œé—´é›†ä¸­ï¼ˆ22:00-02:00ï¼‰
        night_hours = [22, 23, 0, 1, 2]
        night_count = sum(hour_counts.get(h, 0) for h in night_hours)

        # åˆ¤æ–­æ˜¯å¦å·¥ä½œæ—¥é›†ä¸­ï¼ˆå‘¨ä¸€è‡³å‘¨äº”ï¼‰
        weekday_count = len(df[df['åˆ›å»ºæ—¶é—´_è§£æ'].dt.dayofweek < 5])
        weekend_count = len(df) - weekday_count

        patterns = []
        if night_count > len(df) * 0.3:
            patterns.append("å¤œé—´é›†ä¸­ï¼ˆ22:00-02:00ï¼‰")
        if weekday_count > weekend_count * 2:
            patterns.append("å·¥ä½œæ—¥é›†ä¸­")
        elif weekend_count > weekday_count * 2:
            patterns.append("å‘¨æœ«é›†ä¸­")

        return "ã€".join(patterns) if patterns else "æ— æ˜æ˜¾è§„å¾‹"

    def _analyze_platform_distribution_enhanced(self, df: pd.DataFrame) -> Dict[str, Any]:
        """å¹³å°åˆ†å¸ƒåˆ†æï¼ˆå¢å¼ºç‰ˆï¼‰"""
        if len(df) == 0:
            return {'distribution': {}, 'analysis': 'æ— æ•°æ®'}

        platform_counts = df['æ¥æº_æ ‡å‡†'].value_counts()

        distribution = {}
        for platform, count in platform_counts.items():
            percentage = (count / len(df)) * 100
            risk_level = 'æé«˜' if percentage > 70 else 'é«˜' if percentage > 30 else 'ä¸­'

            # è·å–è¯¥å¹³å°çš„é£é™©åˆ†
            platform_df = df[df['æ¥æº_æ ‡å‡†'] == platform]
            avg_risk = platform_df['é£é™©åˆ†_æ•°å€¼'].mean()

            distribution[platform] = {
                'count': int(count),
                'percentage': round(percentage, 1),
                'risk_level': risk_level,
                'avg_risk_score': round(avg_risk, 1),
                'characteristics': self._get_platform_characteristics(platform)
            }

        # ä¸»å¯¼å¹³å°
        dominant = platform_counts.index[0] if len(platform_counts) > 0 else "æ— "
        dominant_ratio = (platform_counts.iloc[0] / len(df)) * 100 if len(platform_counts) > 0 else 0

        analysis = f"ä¸»å¯¼å¹³å°ï¼š{dominant}ï¼ˆå {dominant_ratio:.1f}%ï¼‰"

        return {
            'distribution': distribution,
            'analysis': analysis,
            'dominant_platform': dominant
        }

    def _get_platform_characteristics(self, platform: str) -> List[str]:
        """è·å–å¹³å°ç‰¹å¾"""
        characteristics = {
            'æŠ–éŸ³': ['ä¼ æ’­é€Ÿåº¦å¿«', 'è§¦è¾¾äººç¾¤å¹¿', 'æƒ…æ„Ÿä¼ æ’­å¼º', 'ç›‘ç®¡åŠ›åº¦è¾ƒå¼±'],
            'å¾®åš': ['è¯é¢˜æ€§å¼º', 'è½¬å‘ä¼ æ’­å¿«', 'èˆ†è®ºå‘é…µè¿…é€Ÿ', 'åª’ä½“å…³æ³¨åº¦é«˜'],
            'å¾®ä¿¡': ['å°é—­ä¼ æ’­', 'åœˆå±‚åŒ–æ˜æ˜¾', 'é•¿å°¾æ•ˆåº”å¼º', 'éš¾ä»¥ç›‘æµ‹'],
            'æ–°é—»ç½‘ç«™': ['æƒå¨æ€§é«˜', 'å½±å“æŒä¹…', 'æœç´¢å¼•æ“æ”¶å½•', 'å…¬ä¿¡åŠ›å¼º'],
            'é»‘çŒ«æŠ•è¯‰': ['æŠ•è¯‰èšé›†åœ°', 'æ¶ˆè´¹è€…ç»´æƒ', 'åª’ä½“å…³æ³¨', 'å®˜æ–¹å›å¤'],
            'ç™¾åº¦è´´å§': ['ç¤¾ç¾¤ä¼ æ’­', 'ç”¨æˆ·è®¨è®º', 'é•¿å°¾æ•ˆåº”', 'æœç´¢å¯è§']
        }
        return characteristics.get(platform, ['ä¸€èˆ¬ä¼ æ’­'])

    def _analyze_type_distribution_enhanced(self, df: pd.DataFrame) -> Dict[str, Any]:
        """ç±»å‹åˆ†å¸ƒåˆ†æï¼ˆå¢å¼ºç‰ˆï¼‰"""
        if len(df) == 0:
            return {'distribution': {}, 'analysis': 'æ— æ•°æ®'}

        # å‡è®¾æœ‰ä¸€ä¸ª"ç±»å‹"å­—æ®µï¼Œå¦‚æœæ²¡æœ‰åˆ™ä»å†…å®¹æ¨æ–­
        if 'ç±»å‹' in df.columns:
            type_counts = df['ç±»å‹'].value_counts()
        else:
            # ä»å†…å®¹æ¨æ–­ç±»å‹
            type_counts = self._infer_event_types(df)

        distribution = {}
        for event_type, count in type_counts.items():
            percentage = (count / len(df)) * 100
            severity = self._get_type_severity(event_type)

            distribution[event_type] = {
                'count': int(count),
                'percentage': round(percentage, 1),
                'severity': severity
            }

        return {
            'distribution': distribution,
            'total_types': len(type_counts)
        }

    def _infer_event_types(self, df: pd.DataFrame) -> pd.Series:
        """ä»å†…å®¹æ¨æ–­äº‹ä»¶ç±»å‹"""
        types = []

        for _, row in df.iterrows():
            content = str(row.get('å†…å®¹', '')) + str(row.get('æ ‡é¢˜', ''))

            if any(keyword in content for keyword in ['æ­»äº¡', 'å»ä¸–', 'æŠ¢æ•‘æ— æ•ˆ', 'æ‰‹æœ¯æ­»äº¡']):
                types.append('åŒ»ç–—è´¨é‡-æ­»äº¡äº‹ä»¶')
            elif any(keyword in content for keyword in ['æŠ•è¯‰', 'æ€åº¦å·®', 'æœåŠ¡å·®']):
                types.append('æœåŠ¡è´¨é‡æŠ•è¯‰')
            elif any(keyword in content for keyword in ['è´¹ç”¨', 'æ”¶è´¹', 'è´µ']):
                types.append('æ”¶è´¹é—®é¢˜')
            elif any(keyword in content for keyword in ['ç­‰å¾…', 'æ’é˜Ÿ', 'æ—¶é—´é•¿']):
                types.append('æµç¨‹é—®é¢˜')
            else:
                types.append('å…¶ä»–')

        return pd.Series(types)

    def _get_type_severity(self, event_type: str) -> str:
        """è·å–ç±»å‹çš„ä¸¥é‡ç¨‹åº¦"""
        if 'æ­»äº¡' in event_type:
            return 'æé«˜'
        elif 'æŠ•è¯‰' in event_type or 'è´¹ç”¨' in event_type:
            return 'é«˜'
        else:
            return 'ä¸­'

    def _analyze_department_distribution_enhanced(self, df: pd.DataFrame) -> Dict[str, Any]:
        """ç§‘å®¤åˆ†å¸ƒåˆ†æï¼ˆå¢å¼ºç‰ˆï¼‰"""
        if len(df) == 0 or 'ç§‘å®¤' not in df.columns:
            return {'distribution': {}, 'high_risk_departments': []}

        department_counts = df['ç§‘å®¤'].value_counts()

        distribution = {}
        high_risk_departments = []

        for dept, count in department_counts.items():
            # è·å–è¯¥ç§‘å®¤çš„å¹³å‡é£é™©åˆ†
            dept_df = df[df['ç§‘å®¤'] == dept]
            avg_risk = dept_df['é£é™©åˆ†_æ•°å€¼'].mean()
            max_risk = dept_df['é£é™©åˆ†_æ•°å€¼'].max()

            risk_level = 'ğŸ”´ æé«˜' if avg_risk >= 80 else 'ğŸŸ  é«˜' if avg_risk >= 60 else 'ğŸŸ¡ ä¸­'

            distribution[dept] = {
                'count': int(count),
                'avg_risk_score': round(avg_risk, 1),
                'max_risk_score': int(max_risk),
                'risk_level': risk_level
            }

            if avg_risk >= 80:
                high_risk_departments.append(dept)

        return {
            'distribution': distribution,
            'high_risk_departments': high_risk_departments,
            'total_departments': len(department_counts)
        }

    def _generate_key_events_enhanced(self, df: pd.DataFrame) -> List[Dict[str, Any]]:
        """ç”Ÿæˆå…³é”®äº‹ä»¶ï¼ˆå¢å¼ºç‰ˆï¼‰"""
        if len(df) == 0:
            return []

        # æŒ‰é£é™©åˆ†æ’åºï¼Œå–å‰5ä¸ªé«˜é£é™©äº‹ä»¶
        high_risk_df = df[df['ä¸¥é‡ç¨‹åº¦'].isin(['high', 'medium'])].sort_values('é£é™©åˆ†_æ•°å€¼', ascending=False)

        # æŒ‰ç›¸ä¼¼åº¦åˆ†ç»„ï¼ˆç®€å•çš„æ ‡é¢˜ç›¸ä¼¼åº¦ï¼‰
        event_groups = self._group_similar_events(high_risk_df)

        key_events = []
        for group_id, group_df in event_groups.items():
            if len(group_df) == 0:
                continue

            # è·å–è¯¥ç»„çš„ä»£è¡¨æ€§äº‹ä»¶
            representative = group_df.iloc[0]

            # æ„å»ºäº‹ä»¶è„‰ç»œ
            timeline = self._build_event_timeline(group_df)

            # ä¼ æ’­åˆ†æ
            spread_analysis = self._analyze_event_spread(group_df)

            # æƒ…æ„Ÿåˆ†æ
            sentiment_analysis = self._analyze_event_sentiment(group_df)

            # å½±å“è¯„ä¼°
            impact_assessment = self._assess_event_impact(group_df)

            # å¤„ç½®å»ºè®®
            recommendations = self._generate_event_recommendations(group_df)

            event = {
                'id': group_id,
                'title': representative.get('æ ‡é¢˜', 'æœªçŸ¥äº‹ä»¶'),
                'overview': {
                    'event_time': self._extract_event_time(group_df),
                    'department': representative.get('ç§‘å®¤', 'æœªçŸ¥'),
                    'platform': representative.get('æ¥æº_æ ‡å‡†', 'æœªçŸ¥'),
                    'severity': representative.get('ä¸¥é‡ç¨‹åº¦', 'unknown'),
                    'risk_score': int(representative.get('é£é™©åˆ†_æ•°å€¼', 0)),
                    'total_mentions': len(group_df)
                },
                'timeline': timeline,
                'spread_analysis': spread_analysis,
                'sentiment_analysis': sentiment_analysis,
                'impact_assessment': impact_assessment,
                'recommendations': recommendations
            }

            key_events.append(event)

            if len(key_events) >= 5:
                break

        return key_events

    def _group_similar_events(self, df: pd.DataFrame) -> Dict[int, pd.DataFrame]:
        """æŒ‰ç›¸ä¼¼åº¦åˆ†ç»„äº‹ä»¶"""
        groups = {}
        group_id = 0

        for idx, row in df.iterrows():
            title = str(row.get('æ ‡é¢˜', ''))

            # æ£€æŸ¥æ˜¯å¦ä¸å·²æœ‰ç»„ç›¸ä¼¼
            matched = False
            for existing_id, existing_df in groups.items():
                existing_title = str(existing_df.iloc[0].get('æ ‡é¢˜', ''))
                if self._are_titles_similar(title, existing_title):
                    groups[existing_id] = pd.concat([existing_df, pd.DataFrame([row])], ignore_index=True)
                    matched = True
                    break

            if not matched:
                groups[group_id] = pd.DataFrame([row])
                group_id += 1

        return groups

    def _are_titles_similar(self, title1: str, title2: str) -> bool:
        """åˆ¤æ–­æ ‡é¢˜æ˜¯å¦ç›¸ä¼¼"""
        # ç®€å•çš„ç›¸ä¼¼åº¦åˆ¤æ–­ï¼šæœ‰3ä¸ªä»¥ä¸Šç›¸åŒçš„è¯
        words1 = set(title1.split())
        words2 = set(title2.split())

        intersection = words1.intersection(words2)
        return len(intersection) >= 3

    def _build_event_timeline(self, df: pd.DataFrame) -> Dict[str, Any]:
        """æ„å»ºäº‹ä»¶æ—¶é—´è½´"""
        df_sorted = df.sort_values('åˆ›å»ºæ—¶é—´_è§£æ')

        stages = {
            'occurrence': [],
            'fermentation': [],
            'outbreak': [],
            'continuation': []
        }

        for _, row in df_sorted.iterrows():
            stage_info = {
                'time': row.get('æ—¶é—´_å­—ç¬¦ä¸²', ''),
                'platform': row.get('æ¥æº_æ ‡å‡†', ''),
                'description': row.get('æ ‡é¢˜', '')[:50]
            }

            # æ ¹æ®æ—¶é—´åˆ¤æ–­é˜¶æ®µï¼ˆç®€åŒ–ç‰ˆï¼‰
            hour = row.get('å°æ—¶', 0)
            if hour < 6:
                stages['occurrence'].append(stage_info)
            elif hour < 12:
                stages['fermentation'].append(stage_info)
            elif hour < 18:
                stages['outbreak'].append(stage_info)
            else:
                stages['continuation'].append(stage_info)

        return stages

    def _analyze_event_spread(self, df: pd.DataFrame) -> Dict[str, Any]:
        """åˆ†æäº‹ä»¶ä¼ æ’­"""
        platforms = df['æ¥æº_æ ‡å‡†'].value_counts().to_dict()

        # ä¼°ç®—ä¼ æ’­è·¯å¾„
        spread_path = []
        for _, row in df.sort_values('åˆ›å»ºæ—¶é—´_è§£æ').iterrows():
            spread_path.append({
                'time': row.get('æ—¶é—´_å­—ç¬¦ä¸²', ''),
                'platform': row.get('æ¥æº_æ ‡å‡†', ''),
                'description': row.get('æ ‡é¢˜', '')[:30]
            })

        # è®¡ç®—å½±å“ä¼°ç®—
        estimated_reach = self._estimate_reach(df)

        return {
            'platforms': platforms,
            'spread_path': spread_path,
            'estimated_reach': estimated_reach,
            'total_mentions': len(df),
            'spread_speed': self._calculate_spread_speed(df)
        }

    def _calculate_spread_speed(self, df: pd.DataFrame) -> str:
        """è®¡ç®—ä¼ æ’­é€Ÿåº¦"""
        if len(df) < 2:
            return "æ— æ³•è®¡ç®—"

        df_sorted = df.sort_values('åˆ›å»ºæ—¶é—´_è§£æ')
        time_diff = (df_sorted.iloc[-1]['åˆ›å»ºæ—¶é—´_è§£æ'] - df_sorted.iloc[0]['åˆ›å»ºæ—¶é—´_è§£æ']).total_seconds() / 3600

        if time_diff <= 0:
            return "ç¬é—´"

        mentions_per_hour = len(df) / time_diff

        if mentions_per_hour > 10:
            return "æå¿«ï¼ˆç—…æ¯’å¼ï¼‰"
        elif mentions_per_hour > 5:
            return "å¾ˆå¿«"
        elif mentions_per_hour > 1:
            return "è¾ƒå¿«"
        else:
            return "ä¸€èˆ¬"

    def _analyze_event_sentiment(self, df: pd.DataFrame) -> Dict[str, Any]:
        """åˆ†æäº‹ä»¶æƒ…æ„Ÿ"""
        # åˆå¹¶æ‰€æœ‰å†…å®¹
        all_content = ' '.join(df['å†…å®¹'].fillna('') + ' ' + df['æ ‡é¢˜'].fillna(''))

        # æƒ…æ„Ÿç»Ÿè®¡
        emotion_counts = Counter()

        if JIEBA_AVAILABLE:
            words = jieba.cut(all_content)
            word_list = list(words)

            # ç»Ÿè®¡æƒ…æ„Ÿè¯
            for emotion, keywords in self.emotion_keywords.items():
                count = sum(1 for word in word_list if word in keywords)
                if count > 0:
                    emotion_counts[emotion] += count

        # æå–é«˜é¢‘å…³é”®è¯
        keywords = self._extract_keywords(all_content, top_n=20)

        # æå–å…¬ä¼—è¯‰æ±‚
        demands = self._extract_demands(all_content)

        return {
            'emotion_distribution': dict(emotion_counts),
            'top_keywords': keywords,
            'public_demands': demands
        }

    def _extract_keywords(self, text: str, top_n: int = 20) -> List[Dict[str, Any]]:
        """æå–å…³é”®è¯"""
        if JIEBA_AVAILABLE:
            words = jieba.cut(text)
            word_freq = Counter(words)

            # è¿‡æ»¤åœç”¨è¯
            stop_words = {'çš„', 'äº†', 'æ˜¯', 'åœ¨', 'æˆ‘', 'æœ‰', 'å’Œ', 'å°±', 'ä¸', 'äºº',
                         'éƒ½', 'ä¸€', 'ä¸€ä¸ª', 'ä¸Š', 'ä¹Ÿ', 'å¾ˆ', 'åˆ°', 'è¯´', 'è¦', 'å»',
                         'ä½ ', 'ä¼š', 'ç€', 'æ²¡æœ‰', 'çœ‹', 'å¥½', 'è‡ªå·±', 'è¿™', 'ä½†'}

            filtered = {k: v for k, v in word_freq.items()
                       if len(k) > 1 and k not in stop_words and v > 1}

            top_keywords = sorted(filtered.items(), key=lambda x: x[1], reverse=True)[:top_n]

            return [{'keyword': k, 'count': v} for k, v in top_keywords]
        else:
            return []

    def _extract_demands(self, text: str) -> List[str]:
        """æå–å…¬ä¼—è¯‰æ±‚"""
        demand_patterns = [
            (r'è¦æ±‚.*?è´£ä»»', 'è¦æ±‚åŒ»é™¢æ‰¿æ‹…è´£ä»»'),
            (r'(è¦æ±‚|è¯·æ±‚).*?è°ƒæŸ¥', 'è¦æ±‚è°ƒæŸ¥äº‹ä»¶çœŸç›¸'),
            (r'(è¦æ±‚|è¯·æ±‚).*?é“æ­‰', 'è¦æ±‚é“æ­‰'),
            (r'(è¦æ±‚|è¯·æ±‚).*?èµ”å¿', 'è¦æ±‚èµ”å¿'),
            (r'(è¦æ±‚|è¯·æ±‚).*?é€€æ¬¾', 'è¦æ±‚é€€æ¬¾'),
            (r'(è¦æ±‚|è¯·æ±‚).*?å…¬å¼€', 'è¦æ±‚å…¬å¼€ä¿¡æ¯'),
            (r'(è¦æ±‚|è¯·æ±‚).*?å¤„ç†', 'è¦æ±‚å¤„ç†ç›¸å…³äººå‘˜'),
        ]

        demands = []
        for pattern, demand in demand_patterns:
            if re.search(pattern, text):
                demands.append(demand)

        return demands

    def _assess_event_impact(self, df: pd.DataFrame) -> Dict[str, Any]:
        """è¯„ä¼°äº‹ä»¶å½±å“"""
        avg_risk = df['é£é™©åˆ†_æ•°å€¼'].mean()
        max_risk = df['é£é™©åˆ†_æ•°å€¼'].max()
        total_mentions = len(df)

        # ç¤¾ä¼šå½±å“
        social_impact = []
        if avg_risk >= 80:
            social_impact.append("ä¸¥é‡æŸå®³åŒ»é™¢å£°èª‰")
        if avg_risk >= 70:
            social_impact.append("å¼•å‘å…¬ä¼—å¯¹åŒ»é™¢æ°´å¹³çš„è´¨ç–‘")
        if total_mentions > 5:
            social_impact.append("å¯èƒ½å¼•å‘åª’ä½“è·Ÿè¿›æŠ¥é“")

        # æ½œåœ¨é£é™©
        potential_risks = []
        if 'æ­»äº¡' in ' '.join(df['å†…å®¹'].fillna('')):
            potential_risks.append("å¯èƒ½å¼•å‘æ³•å¾‹è¯‰è®¼")
            potential_risks.append("å¯èƒ½å½±å“åŒ»é™¢è¯„çº§")
        if avg_risk >= 70:
            potential_risks.append("å¯èƒ½å¯¼è‡´å…¶ä»–æ‚£è€…æµå¤±")

        return {
            'social_impact': social_impact,
            'potential_risks': potential_risks,
            'legal_risk': 'é«˜' if 'æ­»äº¡' in ' '.join(df['å†…å®¹'].fillna('')) else 'ä¸­',
            'media_risk': 'é«˜' if total_mentions > 5 else 'ä¸­'
        }

    def _generate_event_recommendations(self, df: pd.DataFrame) -> Dict[str, List[str]]:
        """ç”Ÿæˆäº‹ä»¶å¤„ç½®å»ºè®®"""
        avg_risk = df['é£é™©åˆ†_æ•°å€¼'].mean()

        immediate = []
        short_term = []
        long_term = []

        if avg_risk >= 80:
            immediate.extend([
                "ç«‹å³å¯åŠ¨å±æœºå…¬å…³å“åº”",
                "å‘å¸ƒå®˜æ–¹å£°æ˜",
                "å¯åŠ¨å†…éƒ¨è°ƒæŸ¥",
                "ä¸»åŠ¨ä¸ç›¸å…³æ–¹æ²Ÿé€š"
            ])

        if avg_risk >= 60:
            immediate.append("å¯†åˆ‡ç›‘æ§èˆ†æƒ…å‘å±•")
            short_term.extend([
                "å‡†å¤‡åª’ä½“åº”å¯¹ææ–™",
                "è¯„ä¼°æ³•å¾‹é£é™©"
            ])

        long_term.extend([
            "æ”¹è¿›ç›¸å…³åŒ»ç–—æµç¨‹",
            "åŠ å¼ºåŒ»æ‚£æ²Ÿé€šåŸ¹è®­",
            "å»ºç«‹å±æœºé¢„è­¦æœºåˆ¶"
        ])

        return {
            'immediate': immediate,
            'short_term': short_term,
            'long_term': long_term
        }

    def _extract_event_time(self, df: pd.DataFrame) -> str:
        """æå–äº‹ä»¶æ—¶é—´"""
        if len(df) == 0:
            return "æœªçŸ¥"

        min_time = df['åˆ›å»ºæ—¶é—´_è§£æ'].min()
        max_time = df['åˆ›å»ºæ—¶é—´_è§£æ'].max()

        if min_time == max_time:
            return min_time.strftime('%Yå¹´%mæœˆ%dæ—¥')
        else:
            return f"{min_time.strftime('%mæœˆ%dæ—¥')}-{max_time.strftime('%mæœˆ%dæ—¥')}"

    def _generate_sentiment_enhanced(self, df: pd.DataFrame) -> Dict[str, Any]:
        """ç”Ÿæˆæƒ…æ„Ÿåˆ†æï¼ˆå¢å¼ºç‰ˆï¼‰"""
        if len(df) == 0:
            return {'emotion_distribution': {}, 'top_keywords': [], 'public_demands': []}

        # åˆå¹¶æ‰€æœ‰å†…å®¹
        all_content = ' '.join(df['å†…å®¹'].fillna('') + ' ' + df['æ ‡é¢˜'].fillna(''))

        # æƒ…æ„Ÿç»Ÿè®¡
        emotion_counts = Counter()

        if JIEBA_AVAILABLE:
            words = jieba.cut(all_content)
            word_list = list(words)

            for emotion, keywords in self.emotion_keywords.items():
                count = sum(1 for word in word_list if word in keywords)
                if count > 0:
                    emotion_counts[emotion] = count

        # æå–å…³é”®è¯
        keywords = self._extract_keywords(all_content, top_n=30)

        # æå–è¯‰æ±‚
        demands = self._extract_demands(all_content)

        return {
            'emotion_distribution': dict(emotion_counts),
            'top_keywords': keywords,
            'public_demands': demands
        }

    def _generate_risk_assessment_enhanced(self, df: pd.DataFrame) -> Dict[str, Any]:
        """ç”Ÿæˆé£é™©è¯„ä¼°ï¼ˆå¢å¼ºç‰ˆï¼‰"""
        if len(df) == 0:
            return {'current_risks': [], 'risk_levels': {}}

        current_risks = []
        risk_levels = {
            'red': [],
            'orange': [],
            'yellow': []
        }

        # æŒ‰ç§‘å®¤åˆ†æé£é™©
        if 'ç§‘å®¤' in df.columns:
            for dept in df['ç§‘å®¤'].unique():
                dept_df = df[df['ç§‘å®¤'] == dept]
                avg_risk = dept_df['é£é™©åˆ†_æ•°å€¼'].mean()
                max_risk = dept_df['é£é™©åˆ†_æ•°å€¼'].max()
                count = len(dept_df)

                if avg_risk >= 80:
                    level = 'red'
                    level_text = 'ğŸ”´ çº¢è‰²é¢„è­¦ï¼ˆæé«˜é£é™©ï¼‰'
                elif avg_risk >= 60:
                    level = 'orange'
                    level_text = 'ğŸŸ  æ©™è‰²é¢„è­¦ï¼ˆé«˜é£é™©ï¼‰'
                else:
                    level = 'yellow'
                    level_text = 'ğŸŸ¡ é»„è‰²é¢„è­¦ï¼ˆä¸­é£é™©ï¼‰'

                risk_info = {
                    'department': dept,
                    'avg_risk_score': round(avg_risk, 1),
                    'max_risk_score': int(max_risk),
                    'event_count': int(count),
                    'level_text': level_text
                }

                current_risks.append(risk_info)
                risk_levels[level].append(dept)

        # æŒ‰äº‹ä»¶ç±»å‹åˆ†æ
        event_type_risks = []
        for event_type in df.get('ç±»å‹', pd.Series()).unique():
            if pd.isna(event_type):
                continue
            type_df = df[df['ç±»å‹'] == event_type]
            avg_risk = type_df['é£é™©åˆ†_æ•°å€¼'].mean()

            event_type_risks.append({
                'type': event_type,
                'avg_risk_score': round(avg_risk, 1),
                'event_count': len(type_df)
            })

        return {
            'current_risks': sorted(current_risks, key=lambda x: x['avg_risk_score'], reverse=True),
            'risk_levels': risk_levels,
            'event_type_risks': event_type_risks,
            'overall_risk_level': self._assess_danger_level(df)
        }

    def _generate_recommendations_enhanced(self, df: pd.DataFrame) -> Dict[str, Any]:
        """ç”Ÿæˆåº”å¯¹å»ºè®®ï¼ˆå¢å¼ºç‰ˆï¼‰"""
        avg_risk = df['é£é™©åˆ†_æ•°å€¼'].mean() if len(df) > 0 else 0

        immediate = []
        short_term = []
        long_term = []

        # ç«‹å³æªæ–½ï¼ˆ24å°æ—¶å†…ï¼‰
        if avg_risk >= 80:
            immediate.extend([
                "ğŸš¨ ç«‹å³å¯åŠ¨å±æœºå…¬å…³å“åº”",
                "ğŸ“¢ å‘å¸ƒå®˜æ–¹å£°æ˜",
                "ğŸ” å¯åŠ¨å†…éƒ¨è°ƒæŸ¥",
                "âš–ï¸ å‡†å¤‡æ³•å¾‹åº”å¯¹",
                "ğŸ¤ ä¸»åŠ¨ä¸ç›¸å…³æ–¹æ²Ÿé€š"
            ])

        # çŸ­æœŸæªæ–½ï¼ˆ1å‘¨å†…ï¼‰
        if avg_risk >= 60:
            short_term.extend([
                "ğŸ“Š å…¬å¸ƒè°ƒæŸ¥ç»“æœ",
                "ğŸ‘¥ å¤„ç†ç›¸å…³è´£ä»»äºº",
                "ğŸ”§ æ•´æ”¹åŒ»ç–—æµç¨‹",
                "ğŸ“š åŠ å¼ºåŒ»æ‚£æ²Ÿé€šåŸ¹è®­",
                "âš ï¸ å»ºç«‹å±æœºé¢„è­¦æœºåˆ¶"
            ])

        # é•¿æœŸæªæ–½ï¼ˆ1-3ä¸ªæœˆï¼‰
        long_term.extend([
            "ğŸ¥ æé«˜åŒ»ç–—è´¨é‡",
            "ğŸ’¬ æ”¹å–„æœåŠ¡æ€åº¦",
            "âš¡ ä¼˜åŒ–æœåŠ¡æµç¨‹",
            "ğŸ”’ åŠ å¼ºå±æœºé¢„é˜²",
            "ğŸ“– å»ºç«‹æŠ•è¯‰å¤„ç†åˆ¶åº¦"
        ])

        # é‡ç‚¹é˜²æ§æ–¹å‘
        prevention = {
            'short_term': [
                "é«˜é£é™©ç§‘å®¤ä¸“é¡¹æ•´æ²»",
                "å±æœºç®¡ç†æœºåˆ¶å»ºè®¾",
                "å…¨é™¢æœåŠ¡è´¨é‡æå‡"
            ],
            'medium_term': [
                "åŒ»æ‚£æ²Ÿé€šåŸ¹è®­",
                "æœåŠ¡æµç¨‹ä¼˜åŒ–",
                "æŠ•è¯‰å¤„ç†æœºåˆ¶"
            ]
        }

        # èˆ†æƒ…ç›‘æµ‹é‡ç‚¹
        monitoring = {
            'keywords': self._generate_monitoring_keywords(df),
            'platforms': list(df['æ¥æº_æ ‡å‡†'].unique()),
            'frequency': 'å®æ—¶ç›‘æµ‹ï¼ˆ7x24å°æ—¶ï¼‰'
        }

        return {
            'immediate_actions': immediate,
            'short_term_actions': short_term,
            'long_term_actions': long_term,
            'prevention': prevention,
            'monitoring': monitoring
        }

    def _generate_monitoring_keywords(self, df: pd.DataFrame) -> List[str]:
        """ç”Ÿæˆç›‘æµ‹å…³é”®è¯"""
        if 'åŒ»é™¢' in df.columns:
            hospital_names = df['åŒ»é™¢'].unique().tolist()
        else:
            hospital_names = []

        keywords = []
        for name in hospital_names[:5]:
            keywords.extend([
                name,
                f"{name} æ­»äº¡",
                f"{name} æŠ•è¯‰",
                f"{name} æ‰‹æœ¯"
            ])

        return keywords[:20]

    def _generate_impact_forecast(self, df: pd.DataFrame) -> Dict[str, Any]:
        """ç”Ÿæˆå½±å“é¢„æµ‹"""
        avg_risk = df['é£é™©åˆ†_æ•°å€¼'].mean() if len(df) > 0 else 0

        # çŸ­æœŸå½±å“ï¼ˆ1-7å¤©ï¼‰
        short_term = []
        if avg_risk >= 70:
            short_term.extend([
                "å¹³å°æŒç»­å‘é…µ",
                "å¯èƒ½å‡ºç°æ›´å¤šç›¸å…³å†…å®¹",
                "åŒ»é™¢ç½‘ç»œè¯„åˆ†ä¸‹é™"
            ])

        # ä¸­æœŸå½±å“ï¼ˆ1-4å‘¨ï¼‰
        medium_term = []
        if avg_risk >= 60:
            medium_term.extend([
                "ä¼ ç»Ÿåª’ä½“å¯èƒ½æŠ¥é“",
                "ç›‘ç®¡éƒ¨é—¨å¯èƒ½å…³æ³¨",
                "å¯èƒ½å¼•å‘æ³•å¾‹è¯‰è®¼"
            ])

        if avg_risk >= 70:
            medium_term.append("å°±è¯Šé‡å¯èƒ½ä¸‹é™10-20%")

        # é•¿æœŸå½±å“ï¼ˆ1-3ä¸ªæœˆï¼‰
        long_term = []
        if avg_risk >= 50:
            long_term.extend([
                "åŒ»é™¢å£°èª‰å—æŸ",
                "å“ç‰Œå½¢è±¡ä¸‹é™",
                "å¸‚åœºä»½é¢æµå¤±"
            ])

        # æ³•å¾‹é£é™©è¯„ä¼°
        legal_risk = {
            'probability': '80%' if 'æ­»äº¡' in ' '.join(df['å†…å®¹'].fillna('')) else '30%',
            'estimated_amount': '50-200ä¸‡' if avg_risk >= 70 else '10-50ä¸‡',
            'description': 'åŒ»ç–—æŸå®³èµ”å¿è¯‰è®¼é£é™©è¾ƒé«˜' if avg_risk >= 70 else 'å­˜åœ¨è¯‰è®¼é£é™©'
        }

        return {
            'short_term': short_term,
            'medium_term': medium_term,
            'long_term': long_term,
            'legal_risk': legal_risk
        }

    def _generate_response_templates(self, df: pd.DataFrame) -> Dict[str, str]:
        """ç”Ÿæˆåº”å¯¹æ¨¡æ¿"""
        hospital_name = df.get('åŒ»é™¢', pd.Series()).iloc[0] if len(df) > 0 and 'åŒ»é™¢' in df.columns else "æˆ‘é™¢"

        # é¦–æ¬¡å›åº”æ¨¡æ¿
        first_response = f"""å…³äºç½‘ä¼ {hospital_name}æ‚£è€…äº‹ä»¶çš„é¦–æ¬¡å›åº”

æˆ‘é™¢å…³æ³¨åˆ°ç½‘ç»œå¹³å°å‡ºç°å…³äºæˆ‘é™¢çš„èˆ†æƒ…ï¼Œå¯¹æ­¤æˆ‘ä»¬æ·±è¡¨å…³åˆ‡ã€‚
åŒ»é™¢å·²ç¬¬ä¸€æ—¶é—´æˆç«‹ä¸“é¡¹è°ƒæŸ¥ç»„ï¼Œå¯¹äº‹ä»¶è¿›è¡Œå…¨é¢è°ƒæŸ¥ã€‚

æˆ‘ä»¬æ‰¿è¯ºï¼š
1. ç§‰æŒå®¢è§‚ã€å…¬æ­£ã€é€æ˜çš„åŸåˆ™
2. å°½å¿«æŸ¥æ˜äº‹å®çœŸç›¸
3. ä¾æ³•ä¾è§„å¤„ç†
4. åŠæ—¶å‘ç¤¾ä¼šå…¬å¸ƒè°ƒæŸ¥è¿›å±•

æ„Ÿè°¢ç¤¾ä¼šå„ç•Œç›‘ç£ã€‚

{hospital_name}
{datetime.now().strftime('%Yå¹´%mæœˆ%dæ—¥')}
"""

        # è°ƒæŸ¥è¿›å±•æ¨¡æ¿
        progress_update = f"""å…³äºæ‚£è€…äº‹ä»¶è°ƒæŸ¥è¿›å±•çš„é€šæŠ¥

è‡ªå¯åŠ¨è°ƒæŸ¥ä»¥æ¥ï¼Œæˆ‘é™¢å·²å®Œæˆä»¥ä¸‹å·¥ä½œï¼š

ä¸€ã€å·²å®Œæˆï¼š
1. å°å­˜å…¨éƒ¨ç—…å†èµ„æ–™
2. è°ƒé˜…ç›¸å…³ç›‘æ§å½•åƒ
3. çº¦è°ˆç›¸å…³åŒ»æŠ¤äººå‘˜
4. ä¸ç›¸å…³æ–¹å–å¾—è”ç³»

äºŒã€æ­£åœ¨è¿›è¡Œï¼š
1. åŒ»ç–—è¿‡ç¨‹è¯„ä¼°
2. ç—…å†èµ„æ–™åˆ†æ
3. ä¸“å®¶è®ºè¯
4. è´£ä»»è®¤å®š

ä¸‰ã€åç»­å®‰æ’ï¼š
1. å°½å¿«å…¬å¸ƒè°ƒæŸ¥ç»“æœ
2. ä¾æ³•ä¾è§„å¤„ç†
3. æ”¹è¿›åŒ»ç–—æœåŠ¡

æ„Ÿè°¢ç¤¾ä¼šå„ç•Œçš„å…³å¿ƒå’Œç›‘ç£ã€‚

{hospital_name}
{datetime.now().strftime('%Yå¹´%mæœˆ%dæ—¥')}
"""

        return {
            'first_response': first_response,
            'progress_update': progress_update
        }

    def _generate_appendix_enhanced(self, df: pd.DataFrame) -> Dict[str, Any]:
        """ç”Ÿæˆé™„å½•ï¼ˆå¢å¼ºç‰ˆï¼‰"""
        if len(df) == 0:
            return {'event_list': [], 'contact_info': {}}

        # å®Œæ•´äº‹ä»¶æ¸…å•
        event_list = []
        for _, row in df.sort_values('åˆ›å»ºæ—¶é—´_è§£æ', ascending=False).iterrows():
            event_list.append({
                'id': row.get('ID', ''),
                'time': row.get('æ—¶é—´_å­—ç¬¦ä¸²', ''),
                'platform': row.get('æ¥æº_æ ‡å‡†', ''),
                'type': row.get('ç±»å‹', 'æœªçŸ¥'),
                'department': row.get('ç§‘å®¤', ''),
                'risk_score': int(row.get('é£é™©åˆ†_æ•°å€¼', 0)),
                'status': row.get('çŠ¶æ€', 'unknown'),
                'title': row.get('æ ‡é¢˜', '')[:50]
            })

        # ä¼ æ’­è·¯å¾„ï¼ˆç®€åŒ–ç‰ˆï¼‰
        spread_path = self._build_spread_path(df)

        # è”ç³»æ–¹å¼
        contact_info = {
            'monitoring_center': {
                'è´Ÿè´£äºº': '[å¡«å†™]',
                'ç”µè¯': '[å¡«å†™]',
                'é‚®ç®±': '[å¡«å†™]'
            },
            'crisis_team': {
                'ç»„é•¿': 'é™¢é•¿',
                'æˆå‘˜': ['åŒ»åŠ¡ç§‘', 'å®£ä¼ ç§‘', 'æ³•åŠ¡ç§‘']
            }
        }

        return {
            'event_list': event_list,
            'spread_path': spread_path,
            'contact_info': contact_info
        }

    def _build_spread_path(self, df: pd.DataFrame) -> List[Dict[str, Any]]:
        """æ„å»ºä¼ æ’­è·¯å¾„"""
        if len(df) == 0:
            return []

        df_sorted = df.sort_values('åˆ›å»ºæ—¶é—´_è§£æ')

        path = []
        for _, row in df_sorted.iterrows():
            path.append({
                'time': row.get('æ—¶é—´_å­—ç¬¦ä¸²', ''),
                'platform': row.get('æ¥æº_æ ‡å‡†', ''),
                'title': row.get('æ ‡é¢˜', '')[:30],
                'description': row.get('å†…å®¹', '')[:50]
            })

        return path[:50]  # æœ€å¤šæ˜¾ç¤º50æ¡

    def generate_markdown_report(self, report_data: Dict[str, Any]) -> str:
        """ç”ŸæˆMarkdownæ ¼å¼æŠ¥å‘Š"""
        lines = []

        # æŠ¥å‘Šæ ‡é¢˜
        lines.append(f"# {report_data['hospital_name']}è´Ÿé¢èˆ†æƒ…ä¸“é¡¹åˆ†ææŠ¥å‘Š\n")
        lines.append(f"**æŠ¥å‘Šå‘¨æœŸï¼š** {report_data['report_date_range']}")
        lines.append(f"**æŠ¥å‘Šæ—¶é—´ï¼š** {report_data['report_date']}")
        lines.append(f"**æŠ¥å‘Šç±»å‹ï¼š** èˆ†æƒ…ä¸“é¡¹æŠ¥å‘Š\n")
        lines.append("---\n")

        # ä¸€ã€æŠ¥å‘Šæ¦‚è¿°
        lines.extend(self._format_summary_section(report_data))

        # äºŒã€èˆ†æƒ…åˆ†å¸ƒåˆ†æ
        lines.extend(self._format_distribution_section(report_data))

        # ä¸‰ã€é‡ç‚¹è´Ÿé¢äº‹ä»¶è¯¦æ
        lines.extend(self._format_key_events_section(report_data))

        # å››ã€æƒ…æ„Ÿåˆ†æä¸å…¬ä¼—å…³åˆ‡
        lines.extend(self._format_sentiment_section(report_data))

        # äº”ã€é£é™©é¢„è­¦ä¸è¯„ä¼°
        lines.extend(self._format_risk_section(report_data))

        # å…­ã€åº”å¯¹æªæ–½ä¸å¤„ç½®æƒ…å†µ
        lines.extend(self._format_recommendations_section(report_data))

        # ä¸ƒã€å½±å“é¢„æµ‹
        lines.extend(self._format_impact_section(report_data))

        # å…«ã€å®˜æ–¹å£°æ˜æ¨¡æ¿
        lines.extend(self._format_templates_section(report_data))

        # ä¹ã€é™„å½•
        lines.extend(self._format_appendix_section(report_data))

        # æŠ¥å‘Šç»“æŸ
        lines.append("\n## æŠ¥å‘Šç»“æŸ\n")
        lines.append(f"**æŠ¥å‘Šç”Ÿæˆæ—¶é—´ï¼š** {report_data['generated_time']}")
        lines.append("**æŠ¥å‘Šæœ‰æ•ˆæœŸï¼š** ç«‹å³æ›´æ–°ï¼ˆå»ºè®®æ¯æ—¥æ›´æ–°ï¼‰\n")

        return '\n'.join(lines)

    def _format_summary_section(self, data: Dict[str, Any]) -> List[str]:
        """æ ¼å¼åŒ–æ¦‚è¿°éƒ¨åˆ†"""
        lines = []
        summary = data['summary']

        lines.append("## ä¸€ã€æŠ¥å‘Šæ¦‚è¿°\n")
        lines.append("### 1.1 èˆ†æƒ…æ€»ä½“æ€åŠ¿\n")
        lines.append(f"**{summary['danger_level']}**\n")

        lines.append("### 1.2 å…³é”®æ•°æ®æ‘˜è¦\n")
        lines.append("| æŒ‡æ ‡ | æ•°å€¼ | è¯´æ˜ |")
        lines.append("|------|------|------|")
        lines.append(f"| **è´Ÿé¢èˆ†æƒ…æ€»æ•°** | {summary['total_events']}æ¡ | å…¨éƒ¨ä¸º{'é«˜é£é™©' if summary['high_risk_events'] > 0 else 'ä¸­ä½é£é™©'} |")
        lines.append(f"| **é«˜å±äº‹ä»¶æ•°é‡** | {summary['high_risk_events']}èµ· | éœ€é‡ç‚¹å…³æ³¨ |")
        lines.append(f"| **å½±å“äººæ•°ä¼°ç®—** | {summary['estimated_reach']} | ä»…ä¼°ç®— |")
        lines.append(f"| **ä¼ æ’­å³°å€¼æ—¶é—´** | {summary['peak_time']} | é›†ä¸­çˆ†å‘æœŸ |")
        lines.append(f"| **æ¶‰åŠå¹³å°** | {summary['platforms']}ä¸ª | {', '.join(list(data.get('overview', {}).get('platform_distribution', {}).keys())[:3])} |")
        lines.append(f"| **å¹³å‡é£é™©åˆ†** | {summary['average_risk_score']}åˆ† | æ»¡åˆ†100åˆ† |\n")

        lines.append("### 1.3 ç¯æ¯”å˜åŒ–\n")
        lines.append(f"- {summary['trend']}\n")
        lines.append("---\n")

        return lines

    def _format_distribution_section(self, data: Dict[str, Any]) -> List[str]:
        """æ ¼å¼åŒ–åˆ†å¸ƒåˆ†æéƒ¨åˆ†"""
        lines = []
        dist = data['distribution']

        lines.append("## äºŒã€èˆ†æƒ…åˆ†å¸ƒåˆ†æ\n")

        # æ—¶é—´åˆ†å¸ƒ
        lines.append("### 2.1 æ—¶é—´åˆ†å¸ƒ\n")
        time_dist = dist.get('time_distribution', {})
        timeline = time_dist.get('timeline', [])

        if timeline:
            lines.append("**æ—¶é—´è½´ï¼š**\n")
            lines.append("```")
            for item in timeline[:10]:
                lines.append(f"{item['date']}: {item['count']}æ¡")
                if item.get('time_slots'):
                    lines.append(f"  æ—¶æ®µ: {', '.join(item['time_slots'][:3])}")
            lines.append("```")

        lines.append(f"\n**å…³é”®å‘ç°ï¼š**")
        lines.append(f"- {time_dist.get('time_pattern', 'æ— æ˜æ˜¾è§„å¾‹')}")
        peak_hours_str = ', '.join([f"{h['hour']}:00" for h in time_dist.get('peak_hours', [])[:3]])
        lines.append(f"- å³°å€¼æ—¶æ®µ: {peak_hours_str}\n")

        # å¹³å°åˆ†å¸ƒ
        lines.append("### 2.2 å¹³å°åˆ†å¸ƒ\n")
        platform_dist = dist.get('platform_distribution', {})
        platforms = platform_dist.get('distribution', {})

        if platforms:
            lines.append("| å¹³å° | æ•°é‡ | å æ¯” | é£é™©ç­‰çº§ |")
            lines.append("|------|------|------|----------|")
            for platform, info in platforms.items():
                lines.append(f"| **{platform}** | {info['count']}æ¡ | {info['percentage']}% | {info['risk_level']} |")

        lines.append(f"\n**{platform_dist.get('analysis', '')}**\n")

        # ç±»å‹åˆ†å¸ƒ
        lines.append("### 2.3 ç±»å‹åˆ†å¸ƒ\n")
        type_dist = dist.get('type_distribution', {}).get('distribution', {})

        if type_dist:
            lines.append("| ç±»å‹ | æ•°é‡ | å æ¯” | ä¸¥é‡ç¨‹åº¦ |")
            lines.append("|------|------|------|----------|")
            for event_type, info in type_dist.items():
                lines.append(f"| **{event_type}** | {info['count']}æ¡ | {info['percentage']}% | {info['severity']} |\n")

        # ç§‘å®¤åˆ†å¸ƒ
        if 'ç§‘å®¤' in data.get('raw_dataframe', pd.DataFrame()).columns:
            lines.append("### 2.4 ç§‘å®¤åˆ†å¸ƒ\n")
            dept_dist = dist.get('department_distribution', {}).get('distribution', {})

            if dept_dist:
                lines.append("| ç§‘å®¤ | äº‹ä»¶æ•° | é£é™©ç­‰çº§ | å¹³å‡é£é™©åˆ† |")
                lines.append("|------|--------|----------|------------|")
                for dept, info in list(dept_dist.items())[:5]:
                    lines.append(f"| **{dept}** | {info['count']} | {info['risk_level']} | {info['avg_risk_score']} |")

                high_risk = dist.get('department_distribution', {}).get('high_risk_departments', [])
                if high_risk:
                    lines.append(f"\n**é‡ç‚¹ç›‘æ§ç§‘å®¤ï¼š** {', '.join([f'**{d}**' for d in high_risk])}\n")

        lines.append("---\n")

        return lines

    def _format_key_events_section(self, data: Dict[str, Any]) -> List[str]:
        """æ ¼å¼åŒ–å…³é”®äº‹ä»¶éƒ¨åˆ†"""
        lines = []
        events = data.get('key_events', [])

        if not events:
            return []

        lines.append("## ä¸‰ã€é‡ç‚¹è´Ÿé¢äº‹ä»¶è¯¦æ\n")

        for idx, event in enumerate(events[:3], 1):
            risk_level = 'ğŸ”´' if event['overview']['severity'] == 'high' else 'ğŸŸ '
            lines.append(f"### {risk_level} äº‹ä»¶{idx}ï¼š{event['title'][:50]}\n")

            # æ¦‚å†µ
            lines.append("#### 3.{}.1 äº‹ä»¶æ¦‚å†µ".format(idx))
            lines.append("| é¡¹ç›® | è¯¦æƒ… |")
            lines.append("|------|------|")
            lines.append(f"| **äº‹ä»¶æ—¶é—´** | {event['overview']['event_time']} |")
            lines.append(f"| **æ¶‰äº‹ç§‘å®¤** | {event['overview']['department']} |")
            lines.append(f"| **é¦–å‘å¹³å°** | {event['overview']['platform']} |")
            lines.append(f"| **é£é™©è¯„åˆ†** | {event['overview']['risk_score']}/100 |")
            lines.append(f"| **ä¼ æ’­æ¬¡æ•°** | {event['overview']['total_mentions']}æ¬¡ |\n")

            # ä¼ æ’­åˆ†æ
            if event.get('spread_analysis'):
                lines.append("#### 3.{}.2 ä¼ æ’­åˆ†æ".format(idx))
                spread = event['spread_analysis']
                lines.append(f"- **ä¼ æ’­é€Ÿåº¦ï¼š** {spread.get('spread_speed', 'æœªçŸ¥')}")
                lines.append(f"- **å½±å“ä¼°ç®—ï¼š** {spread.get('estimated_reach', 'æœªçŸ¥')}")
                lines.append(f"- **æ¶‰åŠå¹³å°ï¼š** {', '.join(spread.get('platforms', {}).keys())}\n")

            # æƒ…æ„Ÿåˆ†æ
            if event.get('sentiment_analysis'):
                lines.append("#### 3.{}.3 æƒ…æ„Ÿå€¾å‘".format(idx))
                sentiment = event['sentiment_analysis']

                if sentiment.get('emotion_distribution'):
                    lines.append("**æƒ…æ„Ÿåˆ†å¸ƒï¼š**")
                    for emotion, count in sentiment['emotion_distribution'].items():
                        lines.append(f"- {emotion}: {count}æ¬¡")

                if sentiment.get('top_keywords'):
                    lines.append("\n**é«˜é¢‘å…³é”®è¯ï¼ˆTOP 10ï¼‰ï¼š**")
                    for kw in sentiment['top_keywords'][:10]:
                        lines.append(f"- {kw['keyword']} ({kw['count']}æ¬¡)")

                if sentiment.get('public_demands'):
                    lines.append("\n**å…¬ä¼—è¯‰æ±‚ï¼š**")
                    for demand in sentiment['public_demands']:
                        lines.append(f"- {demand}")
                lines.append("")

            # å¤„ç½®å»ºè®®
            if event.get('recommendations'):
                lines.append("#### 3.{}.4 å¤„ç½®å»ºè®®".format(idx))
                recs = event['recommendations']

                if recs.get('immediate'):
                    lines.append("**ç«‹å³æªæ–½ï¼š**")
                    for rec in recs['immediate']:
                        lines.append(f"- {rec}")

                if recs.get('short_term'):
                    lines.append("\n**çŸ­æœŸæªæ–½ï¼š**")
                    for rec in recs['short_term']:
                        lines.append(f"- {rec}")
                lines.append("")

            lines.append("---\n")

        return lines

    def _format_sentiment_section(self, data: Dict[str, Any]) -> List[str]:
        """æ ¼å¼åŒ–æƒ…æ„Ÿåˆ†æéƒ¨åˆ†"""
        lines = []
        sentiment = data.get('sentiment', {})

        lines.append("## å››ã€æƒ…æ„Ÿåˆ†æä¸å…¬ä¼—å…³åˆ‡\n")

        # æƒ…æ„Ÿåˆ†å¸ƒ
        lines.append("### 4.1 æƒ…æ„Ÿå€¾å‘åˆ†æ\n")
        emotion_dist = sentiment.get('emotion_distribution', {})

        if emotion_dist:
            lines.append("| æƒ…æ„Ÿ | å æ¯” | å…¸å‹è¡¨è¿° |")
            lines.append("|------|------|----------|")
            total = sum(emotion_dist.values())
            for emotion, count in emotion_dist.items():
                percentage = (count / total * 100) if total > 0 else 0
                lines.append(f"| {emotion} | {percentage:.1f}% | è§å…³é”®è¯ |")
        else:
            lines.append("æš‚æ— è¯¦ç»†æƒ…æ„Ÿåˆ†ææ•°æ®\n")

        # å…³é”®è¯
        lines.append("\n### 4.2 å…³é”®è¯äº‘å›¾\n")
        keywords = sentiment.get('top_keywords', [])

        if keywords:
            lines.append("**é«˜é¢‘è¯ï¼ˆTOP 20ï¼‰ï¼š**\n")
            lines.append("```")
            for kw in keywords[:20]:
                lines.append(f"{kw['keyword']} ({kw['count']}æ¬¡)")
            lines.append("```")

        # å…¬ä¼—è¯‰æ±‚
        lines.append("\n### 4.3 å…¬ä¼—ä¸»è¦è¯‰æ±‚\n")
        demands = sentiment.get('public_demands', [])

        if demands:
            for idx, demand in enumerate(demands, 1):
                lines.append(f"{idx}. {demand}")
        else:
            lines.append("æš‚æ— æ˜ç¡®è¯‰æ±‚")

        lines.append("\n---\n")

        return lines

    def _format_risk_section(self, data: Dict[str, Any]) -> List[str]:
        """æ ¼å¼åŒ–é£é™©è¯„ä¼°éƒ¨åˆ†"""
        lines = []
        risk = data.get('risk_assessment', {})

        lines.append("## äº”ã€é£é™©é¢„è­¦ä¸è¯„ä¼°\n")

        # å½“å‰é£é™©ç‚¹
        lines.append("### 5.1 å½“å‰é£é™©ç‚¹\n")
        current_risks = risk.get('current_risks', [])

        if current_risks:
            lines.append("| é£é™©ç‚¹ | ç­‰çº§ | å¹³å‡é£é™©åˆ† | äº‹ä»¶æ•° |")
            lines.append("|--------|------|-----------|--------|")
            for r in current_risks[:5]:
                lines.append(f"| {r['department']} | {r['level_text']} | {r['avg_risk_score']} | {r['event_count']} |")
        else:
            lines.append("æš‚æ— é£é™©ç‚¹\n")

        # é£é™©ç­‰çº§ç»Ÿè®¡
        lines.append("\n### 5.2 é£é™©ç­‰çº§ç»Ÿè®¡\n")
        risk_levels = risk.get('risk_levels', {})

        if risk_levels.get('red'):
            lines.append(f"**ğŸ”´ çº¢è‰²é¢„è­¦ï¼ˆæé«˜ï¼‰ï¼š** {', '.join(risk_levels['red'])}")
        if risk_levels.get('orange'):
            lines.append(f"**ğŸŸ  æ©™è‰²é¢„è­¦ï¼ˆé«˜ï¼‰ï¼š** {', '.join(risk_levels['orange'])}")
        if risk_levels.get('yellow'):
            lines.append(f"**ğŸŸ¡ é»„è‰²é¢„è­¦ï¼ˆä¸­ï¼‰ï¼š** {', '.join(risk_levels['yellow'])}")

        lines.append("\n---\n")

        return lines

    def _format_recommendations_section(self, data: Dict[str, Any]) -> List[str]:
        """æ ¼å¼åŒ–å»ºè®®éƒ¨åˆ†"""
        lines = []
        recs = data.get('recommendations', {})

        lines.append("## å…­ã€åº”å¯¹æªæ–½ä¸å¤„ç½®å»ºè®®\n")

        # ç«‹å³æªæ–½
        immediate = recs.get('immediate_actions', [])
        if immediate:
            lines.append("### 6.1 ç«‹å³åº”å¯¹æªæ–½ï¼ˆ24å°æ—¶å†…ï¼‰\n")
            for action in immediate:
                lines.append(f"{action}")

        # çŸ­æœŸæªæ–½
        short_term = recs.get('short_term_actions', [])
        if short_term:
            lines.append("\n### 6.2 çŸ­æœŸåº”å¯¹æªæ–½ï¼ˆ1å‘¨å†…ï¼‰\n")
            for action in short_term:
                lines.append(f"{action}")

        # é•¿æœŸæªæ–½
        long_term = recs.get('long_term_actions', [])
        if long_term:
            lines.append("\n### 6.3 é•¿æœŸé¢„é˜²æªæ–½ï¼ˆ1-3ä¸ªæœˆï¼‰\n")
            for action in long_term:
                lines.append(f"{action}")

        # ç›‘æµ‹é‡ç‚¹
        monitoring = recs.get('monitoring', {})
        if monitoring:
            lines.append("\n### 6.4 èˆ†æƒ…ç›‘æµ‹é‡ç‚¹\n")
            lines.append(f"**ç›‘æµ‹é¢‘ç‡ï¼š** {monitoring.get('frequency', '')}")

            keywords = monitoring.get('keywords', [])
            if keywords:
                lines.append("\n**å…³é”®è¯ï¼š**")
                for kw in keywords[:10]:
                    lines.append(f"- {kw}")

        lines.append("\n---\n")

        return lines

    def _format_impact_section(self, data: Dict[str, Any]) -> List[str]:
        """æ ¼å¼åŒ–å½±å“é¢„æµ‹éƒ¨åˆ†"""
        lines = []
        impact = data.get('impact_forecast', {})

        lines.append("## ä¸ƒã€å½±å“é¢„æµ‹\n")

        # çŸ­æœŸ
        short = impact.get('short_term', [])
        if short:
            lines.append("### 7.1 çŸ­æœŸå½±å“ï¼ˆ1-7å¤©ï¼‰\n")
            for item in short:
                lines.append(f"- {item}")

        # ä¸­æœŸ
        medium = impact.get('medium_term', [])
        if medium:
            lines.append("\n### 7.2 ä¸­æœŸå½±å“ï¼ˆ1-4å‘¨ï¼‰\n")
            for item in medium:
                lines.append(f"- {item}")

        # é•¿æœŸ
        long = impact.get('long_term', [])
        if long:
            lines.append("\n### 7.3 é•¿æœŸå½±å“ï¼ˆ1-3ä¸ªæœˆï¼‰\n")
            for item in long:
                lines.append(f"- {item}")

        # æ³•å¾‹é£é™©
        legal = impact.get('legal_risk', {})
        if legal:
            lines.append("\n### 7.4 æ³•å¾‹é£é™©è¯„ä¼°\n")
            lines.append(f"- **è¯‰è®¼æ¦‚ç‡ï¼š** {legal.get('probability', '')}")
            lines.append(f"- **é¢„ä¼°é‡‘é¢ï¼š** {legal.get('estimated_amount', '')}")
            lines.append(f"- **é£é™©è¯´æ˜ï¼š** {legal.get('description', '')}")

        lines.append("\n---\n")

        return lines

    def _format_templates_section(self, data: Dict[str, Any]) -> List[str]:
        """æ ¼å¼åŒ–å£°æ˜æ¨¡æ¿éƒ¨åˆ†"""
        lines = []
        templates = data.get('response_templates', {})

        lines.append("## å…«ã€å®˜æ–¹å£°æ˜æ¨¡æ¿\n")

        # é¦–æ¬¡å›åº”
        first = templates.get('first_response', '')
        if first:
            lines.append("### 8.1 é¦–æ¬¡å›åº”æ¨¡æ¿\n")
            lines.append("```")
            lines.append(first)
            lines.append("```")

        # è¿›å±•é€šæŠ¥
        progress = templates.get('progress_update', '')
        if progress:
            lines.append("\n### 8.2 è°ƒæŸ¥è¿›å±•æ¨¡æ¿\n")
            lines.append("```")
            lines.append(progress)
            lines.append("```")

        lines.append("\n---\n")

        return lines

    def _format_appendix_section(self, data: Dict[str, Any]) -> List[str]:
        """æ ¼å¼åŒ–é™„å½•éƒ¨åˆ†"""
        lines = []
        appendix = data.get('appendix', {})

        lines.append("## ä¹ã€é™„å½•\n")

        # äº‹ä»¶æ¸…å•
        event_list = appendix.get('event_list', [])
        if event_list:
            lines.append("### é™„å½•Aï¼šè´Ÿé¢èˆ†æƒ…äº‹ä»¶æ¸…å•\n")
            lines.append("| ID | æ—¶é—´ | å¹³å° | ç±»å‹ | ç§‘å®¤ | é£é™©åˆ† | çŠ¶æ€ |")
            lines.append("|----|------|------|------|------|--------|------|")
            for event in event_list[:20]:
                lines.append(f"| {event['id']} | {event['time'][:10]} | {event['platform']} | {event['type']} | {event['department']} | {event['risk_score']} | {event['status']} |")

        # è”ç³»æ–¹å¼
        contact = appendix.get('contact_info', {})
        if contact:
            lines.append("\n### é™„å½•Bï¼šè”ç³»æ–¹å¼\n")
            monitoring = contact.get('monitoring_center', {})
            if monitoring:
                lines.append("**èˆ†æƒ…ç›‘æµ‹ä¸­å¿ƒï¼š**")
                lines.append(f"- è´Ÿè´£äººï¼š{monitoring.get('è´Ÿè´£äºº', '')}")
                lines.append(f"- ç”µè¯ï¼š{monitoring.get('ç”µè¯', '')}")
                lines.append(f"- é‚®ç®±ï¼š{monitoring.get('é‚®ç®±', '')}")

            crisis = contact.get('crisis_team', {})
            if crisis:
                lines.append("\n**å±æœºå…¬å…³å°ç»„ï¼š**")
                lines.append(f"- ç»„é•¿ï¼š{crisis.get('ç»„é•¿', '')}")
                lines.append(f"- æˆå‘˜ï¼š{', '.join(crisis.get('æˆå‘˜', []))}")

        lines.append("\n---\n")

        return lines
