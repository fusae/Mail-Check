#!/usr/bin/env python3
"""
èˆ†æƒ…æŠ¥å‘Šç”Ÿæˆå™¨æ ¸å¿ƒæ¨¡å—
Sentiment Analysis Report Generator Core
"""

import pandas as pd
import numpy as np
from datetime import datetime
from typing import Dict, List, Any
from collections import Counter
import re

try:
    import jieba
    JIEBA_AVAILABLE = True
except ImportError:
    JIEBA_AVAILABLE = False

try:
    from docx import Document
    from docx.shared import Inches, Pt, RGBColor
    from docx.enum.text import WD_ALIGN_PARAGRAPH
    DOCX_AVAILABLE = True
except ImportError:
    DOCX_AVAILABLE = False


class ReportGenerator:
    """èˆ†æƒ…æŠ¥å‘Šç”Ÿæˆå™¨"""

    def __init__(self):
        self.platform_names = {
            'æŠ–éŸ³': 'æŠ–éŸ³',
            'douyin': 'æŠ–éŸ³',
            'æ–°æµªå¾®åš': 'å¾®åš',
            'å¾®åš': 'å¾®åš',
            'weibo': 'å¾®åš',
            'å¾®ä¿¡': 'å¾®ä¿¡',
            'wechat': 'å¾®ä¿¡',
            'æ–°é—»ç½‘ç«™': 'æ–°é—»ç½‘ç«™',
            'news': 'æ–°é—»ç½‘ç«™',
        }
        self.max_key_events = 10
        self.event_reason_limit = 400
        self.event_content_limit = 800

    def normalize_platform(self, platform: str) -> str:
        """æ ‡å‡†åŒ–å¹³å°åç§°"""
        platform = platform.lower().strip()
        for key, value in self.platform_names.items():
            if key in platform:
                return value
        return platform

    def generate_report_data(
        self,
        df: pd.DataFrame,
        hospital_name: str,
        report_type: str = "special",
        report_period: str = None
    ) -> Dict[str, Any]:
        """
        ç”ŸæˆæŠ¥å‘Šæ•°æ®

        å‚æ•°ï¼š
        - df: èˆ†æƒ…æ•°æ®DataFrame
        - hospital_name: åŒ»é™¢åç§°
        - report_type: æŠ¥å‘Šç±»å‹ï¼ˆspecial/quarterly/monthlyï¼‰
        - report_period: æŠ¥å‘Šå‘¨æœŸï¼ˆå¦‚"2026Q1"ï¼‰
        """
        # æ•°æ®é¢„å¤„ç†
        df = self._preprocess_data(df)

        # ç”Ÿæˆå„ä¸ªéƒ¨åˆ†çš„æ•°æ®
        report_data = {
            'hospital_name': hospital_name,
            'report_type': report_type,
            'report_period': report_period or self._auto_detect_period(df),
            'generated_time': datetime.now().strftime('%Yå¹´%mæœˆ%dæ—¥ %H:%M'),
            'summary': self._generate_summary(df),
            'overview': self._generate_overview(df),
            'distribution': self._generate_distribution(df),
            'key_events': self._generate_key_events(df),
            'sentiment': self._generate_sentiment(df),
            'risk_assessment': self._generate_risk_assessment(df),
            'recommendations': self._generate_recommendations(df),
            'appendix': self._generate_appendix(df),
            'raw_dataframe': df  # ä¿å­˜åŸå§‹æ•°æ®ä¾›è°ƒè¯•ä½¿ç”¨
        }

        return report_data

    def _preprocess_data(self, df: pd.DataFrame) -> pd.DataFrame:
        """æ•°æ®é¢„å¤„ç†"""
        # å¤åˆ¶æ•°æ®
        df = df.copy()

        # æ ‡å‡†åŒ–å¹³å°åç§°
        df['æ¥æº_æ ‡å‡†'] = df['æ¥æº'].apply(self.normalize_platform)

        # è§£ææ—¶é—´
        df['åˆ›å»ºæ—¶é—´_è§£æ'] = pd.to_datetime(df['åˆ›å»ºæ—¶é—´'], errors='coerce')

        # æå–æ—¥æœŸå’Œå°æ—¶
        df['æ—¥æœŸ'] = df['åˆ›å»ºæ—¶é—´_è§£æ'].dt.date
        df['å°æ—¶'] = df['åˆ›å»ºæ—¶é—´_è§£æ'].dt.hour

        # è®¡ç®—ç¯æ¯”ï¼ˆå¦‚æœæœ‰å†å²æ•°æ®ï¼‰
        df['é£é™©åˆ†_æ•°å€¼'] = pd.to_numeric(df['é£é™©åˆ†'], errors='coerce').fillna(0)

        return df

    def _generate_summary(self, df: pd.DataFrame) -> Dict[str, Any]:
        """ç”ŸæˆæŠ¥å‘Šæ‘˜è¦"""
        total = len(df)
        high_risk = len(df[df['ä¸¥é‡ç¨‹åº¦'] == 'high'])
        active = len(df[df['çŠ¶æ€'] == 'active'])
        avg_risk = df['é£é™©åˆ†_æ•°å€¼'].mean()

        # ä¼°ç®—å½±å“äººæ•°ï¼ˆæ ¹æ®å¹³å°å’Œä¸¥é‡ç¨‹åº¦ï¼‰
        estimated_reach = self._estimate_reach(df)

        return {
            'total_events': total,
            'high_risk_events': high_risk,
            'active_events': active,
            'average_risk_score': round(avg_risk, 1),
            'estimated_reach': estimated_reach,
            'peak_time': self._find_peak_time(df),
            'trend': self._analyze_trend(df)
        }

    def _estimate_reach(self, df: pd.DataFrame) -> str:
        """ä¼°ç®—å½±å“äººæ•°"""
        # ç®€å•ä¼°ç®—ï¼šæ¯æ¡é«˜é£é™©=10ä¸‡ï¼Œä¸­é£é™©=1ä¸‡ï¼Œä½é£é™©=1000
        high_count = len(df[df['ä¸¥é‡ç¨‹åº¦'] == 'high'])
        medium_count = len(df[df['ä¸¥é‡ç¨‹åº¦'] == 'medium'])
        low_count = len(df[df['ä¸¥é‡ç¨‹åº¦'] == 'low'])

        total = high_count * 100000 + medium_count * 10000 + low_count * 1000

        if total >= 1000000:
            return f"{total // 1000000}M+"
        elif total >= 10000:
            return f"{total // 10000}ä¸‡+"
        else:
            return f"{total // 1000}åƒ+"

    def _find_peak_time(self, df: pd.DataFrame) -> str:
        """æ‰¾åˆ°ä¼ æ’­å³°å€¼æ—¶é—´"""
        if len(df) == 0:
            return "æœªçŸ¥"

        # æŒ‰æ—¥æœŸåˆ†ç»„
        daily_counts = df.groupby('æ—¥æœŸ').size()
        if len(daily_counts) == 0:
            return "æœªçŸ¥"

        peak_date = daily_counts.idxmax()
        return peak_date.strftime('%Y-%m-%d')

    def _analyze_trend(self, df: pd.DataFrame) -> str:
        """åˆ†æè¶‹åŠ¿"""
        if len(df) < 2:
            return "æ•°æ®ä¸è¶³"

        # æŒ‰æ—¥æœŸæ’åº
        df_sorted = df.sort_values('åˆ›å»ºæ—¶é—´_è§£æ')

        # ç®€å•åˆ¤æ–­è¶‹åŠ¿
        first_half = df_sorted[:len(df_sorted)//2]
        second_half = df_sorted[len(df_sorted)//2:]

        first_count = len(first_half)
        second_count = len(second_half)

        if second_count > first_count * 1.5:
            return "ä¸Šå‡"
        elif second_count < first_count * 0.7:
            return "ä¸‹é™"
        else:
            return "å¹³ç¨³"

    def _generate_overview(self, df: pd.DataFrame) -> Dict[str, Any]:
        """ç”Ÿæˆæ¦‚è¿°æ•°æ®"""
        total = len(df)

        # æŒ‰ä¸¥é‡ç¨‹åº¦ç»Ÿè®¡
        severity_counts = df['ä¸¥é‡ç¨‹åº¦'].value_counts()

        # æŒ‰çŠ¶æ€ç»Ÿè®¡
        status_counts = df['çŠ¶æ€'].value_counts()

        # æŒ‰å¹³å°ç»Ÿè®¡
        platform_counts = df['æ¥æº_æ ‡å‡†'].value_counts()

        return {
            'total': total,
            'severity_distribution': {
                'high': int(severity_counts.get('high', 0)),
                'medium': int(severity_counts.get('medium', 0)),
                'low': int(severity_counts.get('low', 0))
            },
            'status_distribution': status_counts.to_dict(),
            'platform_distribution': platform_counts.to_dict(),
            'average_risk_score': round(df['é£é™©åˆ†_æ•°å€¼'].mean(), 1)
        }

    def _generate_distribution(self, df: pd.DataFrame) -> Dict[str, Any]:
        """ç”Ÿæˆåˆ†å¸ƒåˆ†æ"""
        return {
            'time_distribution': self._analyze_time_distribution(df),
            'platform_distribution': self._analyze_platform_distribution(df),
            'type_distribution': self._analyze_type_distribution(df),
            'department_distribution': self._analyze_department_distribution(df)
        }

    def _analyze_time_distribution(self, df: pd.DataFrame) -> Dict[str, Any]:
        """æ—¶é—´åˆ†å¸ƒåˆ†æ"""
        # æŒ‰æ—¥æœŸç»Ÿè®¡
        daily_counts = df.groupby('æ—¥æœŸ').size().to_dict()

        # æŒ‰å°æ—¶ç»Ÿè®¡
        hourly_counts = df.groupby('å°æ—¶').size().to_dict()

        # æ‰¾å‡ºå‘ç—…æ—¶æ®µ
        peak_hours = sorted(hourly_counts.items(), key=lambda x: x[1], reverse=True)[:3]

        return {
            'daily_counts': {str(k): v for k, v in daily_counts.items()},
            'hourly_counts': hourly_counts,
            'peak_hours': peak_hours,
            'time_pattern': self._detect_time_pattern(df)
        }

    def _detect_time_pattern(self, df: pd.DataFrame) -> str:
        """æ£€æµ‹æ—¶é—´æ¨¡å¼"""
        if len(df) == 0:
            return "æ— æ•°æ®"

        hour_counts = df.groupby('å°æ—¶').size()

        # åˆ¤æ–­æ˜¯å¦å¤œé—´é›†ä¸­ï¼ˆ22:00-02:00ï¼‰
        night_hours = [22, 23, 0, 1, 2]
        night_count = sum(hour_counts.get(h, 0) for h in night_hours)

        if night_count > len(df) * 0.3:
            return "å¤œé—´é›†ä¸­ï¼ˆ22:00-02:00ï¼‰"
        else:
            return "åˆ†æ•£"

    def _analyze_platform_distribution(self, df: pd.DataFrame) -> Dict[str, Any]:
        """å¹³å°åˆ†å¸ƒåˆ†æ"""
        platform_counts = df['æ¥æº_æ ‡å‡†'].value_counts()
        total = len(df)

        distribution = {}
        for platform, count in platform_counts.items():
            distribution[platform] = {
                'count': int(count),
                'percentage': round(count / total * 100, 1)
            }

        return {
            'distribution': distribution,
            'dominant_platform': platform_counts.idxmax() if len(platform_counts) > 0 else "æœªçŸ¥",
            'platform_risk': self._assess_platform_risk(df)
        }

    def _assess_platform_risk(self, df: pd.DataFrame) -> Dict[str, str]:
        """è¯„ä¼°å¹³å°é£é™©ç­‰çº§"""
        platform_risk = {
            'æŠ–éŸ³': 'æé«˜',
            'å¾®åš': 'é«˜',
            'å¾®ä¿¡': 'é«˜',
            'æ–°é—»ç½‘ç«™': 'ä¸­é«˜'
        }

        result = {}
        for platform in df['æ¥æº_æ ‡å‡†'].unique():
            result[platform] = platform_risk.get(platform, 'ä¸­')

        return result

    def _analyze_type_distribution(self, df: pd.DataFrame) -> Dict[str, Any]:
        """ç±»å‹åˆ†å¸ƒåˆ†æ"""
        # ä»è­¦ç¤ºç†ç”±å’Œå†…å®¹ä¸­æå–ç±»å‹
        types = []

        for _, row in df.iterrows():
            reason = str(row.get('è­¦ç¤ºç†ç”±', '')).lower()
            content = str(row.get('å†…å®¹', '')).lower()

            if any(keyword in reason or keyword in content for keyword in ['æ­»äº¡', 'æ­»äº¡äº‹ä»¶', 'è‡´æ­»', 'æ²»æ­»']):
                types.append('åŒ»ç–—è´¨é‡-æ­»äº¡äº‹ä»¶')
            elif any(keyword in reason or keyword in content for keyword in ['æ‰‹æœ¯', 'æ²»ç–—', 'è¯Šæ–­']):
                types.append('åŒ»ç–—è´¨é‡-æ‰‹æœ¯/æ²»ç–—')
            elif any(keyword in reason or keyword in content for keyword in ['æœåŠ¡', 'æ€åº¦', 'æŠ•è¯‰']):
                types.append('æœåŠ¡è´¨é‡')
            elif any(keyword in reason or keyword in content for keyword in ['è´¹ç”¨', 'æ”¶è´¹', 'é’±']):
                types.append('è´¹ç”¨ç›¸å…³')
            elif any(keyword in reason or keyword in content for keyword in ['ç¯å¢ƒ', 'è®¾æ–½', 'åœè½¦']):
                types.append('ç¯å¢ƒè®¾æ–½')
            else:
                types.append('å…¶ä»–')

        type_counts = Counter(types)
        total = len(types)

        distribution = {}
        for type_name, count in type_counts.items():
            distribution[type_name] = {
                'count': count,
                'percentage': round(count / total * 100, 1)
            }

        return {
            'distribution': distribution,
            'main_type': type_counts.most_common(1)[0][0] if type_counts else "æœªçŸ¥"
        }

    def _analyze_department_distribution(self, df: pd.DataFrame) -> Dict[str, Any]:
        """ç§‘å®¤åˆ†å¸ƒåˆ†æ"""
        # ä»å†…å®¹ä¸­æå–ç§‘å®¤
        departments = []

        dept_keywords = {
            'å¿ƒå†…ç§‘': ['å¿ƒå†…', 'å¿ƒè„', 'å¿ƒç§‘'],
            'å¿ƒå¤–ç§‘': ['å¿ƒå¤–'],
            'æ€¥è¯Šç§‘': ['æ€¥è¯Š'],
            'äº§ç§‘': ['äº§ç§‘', 'ç”Ÿäº§'],
            'å„¿ç§‘': ['å„¿ç§‘', 'å°å„¿'],
            'è€³é¼»å–‰ç§‘': ['è€³é¼»å–‰', 'è€³é¼»'],
            'éª¨ç§‘': ['éª¨ç§‘'],
            'å¤–ç§‘': ['å¤–ç§‘']
        }

        for _, row in df.iterrows():
            content = str(row.get('å†…å®¹', '')) + str(row.get('è­¦ç¤ºç†ç”±', ''))
            found = False

            for dept, keywords in dept_keywords.items():
                if any(keyword in content for keyword in keywords):
                    departments.append(dept)
                    found = True
                    break

            if not found:
                departments.append('å…¶ä»–/æœªæ˜ç¡®')

        dept_counts = Counter(departments)

        return {
            'department_counts': dict(dept_counts),
            'high_risk_departments': dept_counts.most_common(3)
        }

    def _generate_key_events(self, df: pd.DataFrame) -> List[Dict[str, Any]]:
        """ç”Ÿæˆé‡ç‚¹äº‹ä»¶"""
        # æŒ‰é£é™©åˆ†æ’åºï¼Œå–å‰Nä¸ª
        top_events = df.nlargest(self.max_key_events, 'é£é™©åˆ†_æ•°å€¼')

        events = []
        for _, row in top_events.iterrows():
            event = {
                'id': row.get('ID', 'N/A'),
                'title': row.get('æ ‡é¢˜', 'æ— æ ‡é¢˜'),
                'platform': row.get('æ¥æº_æ ‡å‡†', row.get('æ¥æº', 'æœªçŸ¥')),
                'severity': row.get('ä¸¥é‡ç¨‹åº¦', 'unknown'),
                'risk_score': int(row.get('é£é™©åˆ†_æ•°å€¼', 0)),
                'status': row.get('çŠ¶æ€', 'unknown'),
                'time': str(row.get('åˆ›å»ºæ—¶é—´', 'æœªçŸ¥')),
                'reason': row.get('è­¦ç¤ºç†ç”±', '') or '',
                'content': row.get('å†…å®¹', '') or '',
                'link': row.get('åŸæ–‡é“¾æ¥', ''),
                'department': self._extract_department(row.get('å†…å®¹', '')),
                'event_type': self._classify_event(row.get('è­¦ç¤ºç†ç”±', ''), row.get('å†…å®¹', ''))
            }
            events.append(event)

        return events

    def _extract_department(self, content: str) -> str:
        """ä»å†…å®¹ä¸­æå–ç§‘å®¤"""
        dept_keywords = {
            'å¿ƒå†…ç§‘': ['å¿ƒå†…', 'å¿ƒè„', 'å¿ƒç§‘'],
            'å¿ƒå¤–ç§‘': ['å¿ƒå¤–'],
            'æ€¥è¯Šç§‘': ['æ€¥è¯Š'],
            'äº§ç§‘': ['äº§ç§‘', 'ç”Ÿäº§'],
            'å„¿ç§‘': ['å„¿ç§‘', 'å°å„¿'],
            'è€³é¼»å–‰ç§‘': ['è€³é¼»å–‰', 'è€³é¼»'],
        }

        content = str(content)
        for dept, keywords in dept_keywords.items():
            if any(keyword in content for keyword in keywords):
                return dept

        return "æœªæ˜ç¡®"

    def _classify_event(self, reason: str, content: str) -> str:
        """äº‹ä»¶åˆ†ç±»"""
        text = (str(reason) + ' ' + str(content)).lower()

        if 'æ­»äº¡' in text or 'è‡´æ­»' in text or 'æ²»æ­»' in text:
            return 'ğŸ”´ æé«˜é£é™© - æ‚£è€…æ­»äº¡'
        elif 'æ‰‹æœ¯' in text:
            return 'ğŸŸ  é«˜é£é™© - æ‰‹æœ¯ç›¸å…³'
        elif 'æŠ•è¯‰' in text:
            return 'ğŸŸ¡ ä¸­é£é™© - æœåŠ¡æŠ•è¯‰'
        else:
            return 'ğŸŸ¢ ä¸€èˆ¬é£é™©'

    def _generate_sentiment(self, df: pd.DataFrame) -> Dict[str, Any]:
        """ç”Ÿæˆæƒ…æ„Ÿåˆ†æ"""
        # ç®€å•æƒ…æ„Ÿåˆ†æï¼ˆåŸºäºå…³é”®è¯ï¼‰
        emotions = {'æ„¤æ€’': 0, 'æ‚²ä¼¤': 0, 'å¤±æœ›': 0, 'è´¨ç–‘': 0, 'æ‹…å¿§': 0}

        keywords = {
            'æ„¤æ€’': ['æ²»æ­»', 'å®³æ­»', 'ä¸è´Ÿè´£ä»»', 'åƒåœ¾', 'æ— è‰¯'],
            'æ‚²ä¼¤': ['å¥½å¥½çš„ä¸€ä¸ªäºº', 'å»ä¸–', 'èµ°äº†', 'éš¾è¿‡'],
            'å¤±æœ›': ['å¤±æœ›', 'ä¸ç›¸ä¿¡', 'æ€€ç–‘'],
            'è´¨ç–‘': ['è´¨ç–‘', 'ä¸ºä»€ä¹ˆ', 'æ€ä¹ˆå›äº‹'],
            'æ‹…å¿§': ['æ‹…å¿ƒ', 'å®³æ€•', 'ææ…Œ']
        }

        for _, row in df.iterrows():
            content = str(row.get('å†…å®¹', '')).lower()

            for emotion, words in keywords.items():
                if any(word in content for word in words):
                    emotions[emotion] += 1

        total = sum(emotions.values())
        sentiment_distribution = {}
        for emotion, count in emotions.items():
            sentiment_distribution[emotion] = {
                'count': count,
                'percentage': round(count / total * 100, 1) if total > 0 else 0
            }

        # æå–å…³é”®è¯
        top_keywords = self._extract_keywords(df, top_n=20)

        return {
            'sentiment_distribution': sentiment_distribution,
            'dominant_emotion': max(emotions, key=emotions.get) if total > 0 else 'æœªçŸ¥',
            'sentiment_intensity': 'æå¼º' if total > len(df) * 0.5 else 'ä¸€èˆ¬',
            'top_keywords': top_keywords
        }

    def _extract_keywords(self, df: pd.DataFrame, top_n: int = 20) -> List[tuple]:
        """æå–é«˜é¢‘å…³é”®è¯"""
        all_text = ' '.join(df['å†…å®¹'].fillna('').astype(str).tolist())

        if JIEBA_AVAILABLE:
            words = jieba.cut(all_text)
            # è¿‡æ»¤åœç”¨è¯
            stopwords = {'çš„', 'äº†', 'æ˜¯', 'æˆ‘', 'ä½ ', 'ä»–', 'å¥¹', 'åœ¨', 'æœ‰', 'å’Œ', 'å°±', 'ä¸', 'äºº', 'éƒ½', 'ä¸€', 'ä¸€ä¸ª', 'ä¸Š', 'ä¹Ÿ', 'å¾ˆ', 'åˆ°', 'è¯´', 'è¦', 'å»', 'ä½ ', 'ä¼š', 'ç€', 'æ²¡æœ‰', 'çœ‹', 'å¥½', 'è‡ªå·±', 'è¿™'}
            words = [w for w in words if len(w) > 1 and w not in stopwords]
        else:
            # ç®€å•åˆ†è¯ï¼ˆæŒ‰ç©ºæ ¼å’Œæ ‡ç‚¹ï¼‰
            import re
            words = re.findall(r'[\w]{2,}', all_text)

        word_counts = Counter(words)
        return word_counts.most_common(top_n)

    def _generate_risk_assessment(self, df: pd.DataFrame) -> Dict[str, Any]:
        """ç”Ÿæˆé£é™©è¯„ä¼°"""
        high_risk = df[df['ä¸¥é‡ç¨‹åº¦'] == 'high']
        medium_risk = df[df['ä¸¥é‡ç¨‹åº¦'] == 'medium']
        low_risk = df[df['ä¸¥é‡ç¨‹åº¦'] == 'low']

        return {
            'current_risks': {
                'critical': {
                    'count': len(high_risk),
                    'events': high_risk.nlargest(3, 'é£é™©åˆ†_æ•°å€¼')[['æ ‡é¢˜', 'é£é™©åˆ†_æ•°å€¼']].to_dict('records') if len(high_risk) > 0 else []
                },
                'high': {
                    'count': len(medium_risk),
                    'events': []
                },
                'medium': {
                    'count': len(low_risk),
                    'events': []
                }
            },
            'risk_level': self._calculate_overall_risk(df),
            'impact_prediction': self._predict_impact(df)
        }

    def _calculate_overall_risk(self, df: pd.DataFrame) -> str:
        """è®¡ç®—æ€»ä½“é£é™©ç­‰çº§"""
        avg_risk = df['é£é™©åˆ†_æ•°å€¼'].mean()
        high_ratio = len(df[df['ä¸¥é‡ç¨‹åº¦'] == 'high']) / len(df)

        if avg_risk >= 80 or high_ratio >= 0.5:
            return 'ğŸ”´ æé«˜å±é™©çº§åˆ«'
        elif avg_risk >= 60 or high_ratio >= 0.3:
            return 'ğŸŸ  é«˜å±é™©çº§åˆ«'
        elif avg_risk >= 40 or high_ratio >= 0.1:
            return 'ğŸŸ¡ ä¸­å±é™©çº§åˆ«'
        else:
            return 'ğŸŸ¢ ä½å±é™©çº§åˆ«'

    def _predict_impact(self, df: pd.DataFrame) -> Dict[str, str]:
        """é¢„æµ‹å½±å“"""
        return {
            'short_term': 'å¹³å°æŒç»­å‘é…µï¼Œå¯èƒ½æ–°å¢ç›¸å…³å†…å®¹',
            'medium_term': 'å¯èƒ½å¼•å‘åª’ä½“å…³æ³¨å’Œç›‘ç®¡ä»‹å…¥',
            'long_term': 'åŒ»é™¢å£°èª‰å—æŸï¼Œéœ€é•¿æœŸä¿®å¤',
            'legal_risk': 'å¯èƒ½é¢ä¸´æ³•å¾‹è¯‰è®¼ï¼Œèµ”å¿é‡‘é¢é¢„ä¼°50-200ä¸‡'
        }

    def _generate_recommendations(self, df: pd.DataFrame) -> Dict[str, Any]:
        """ç”Ÿæˆå»ºè®®"""
        return {
            'immediate_actions': [
                'ğŸš¨ ç«‹å³å¯åŠ¨å±æœºå…¬å…³å“åº”',
                'ğŸ“¢ 24å°æ—¶å†…å‘å¸ƒå®˜æ–¹å£°æ˜',
                'ğŸ” å¯åŠ¨å†…éƒ¨è°ƒæŸ¥',
                'ğŸ¤ ä¸»åŠ¨ä¸ç›¸å…³æ–¹æ²Ÿé€š'
            ],
            'short_term_actions': [
                'å…¬å¸ƒè°ƒæŸ¥è¿›å±•',
                'å¤„ç†ç›¸å…³è´£ä»»äºº',
                'æ•´æ”¹åŒ»ç–—æµç¨‹'
            ],
            'long_term_actions': [
                'å»ºç«‹å±æœºé¢„è­¦æœºåˆ¶',
                'åŠ å¼ºåŒ»æ‚£æ²Ÿé€šåŸ¹è®­',
                'æå‡æœåŠ¡è´¨é‡',
                'å®šæœŸèˆ†æƒ…ç›‘æµ‹'
            ],
            'monitoring_focus': [
                'æŠ–éŸ³å¹³å°ï¼ˆæœ€é«˜ä¼˜å…ˆçº§ï¼‰',
                'å¾®åš',
                'å¾®ä¿¡',
                'åœ°æ–¹è®ºå›'
            ]
        }

    def _generate_appendix(self, df: pd.DataFrame) -> Dict[str, Any]:
        """ç”Ÿæˆé™„å½•æ•°æ®"""
        # ç¡®ä¿å¿…è¦çš„åˆ—å­˜åœ¨
        columns_to_export = ['åˆ›å»ºæ—¶é—´', 'æ¥æº', 'ä¸¥é‡ç¨‹åº¦', 'é£é™©åˆ†', 'çŠ¶æ€']
        if 'ID' in df.columns:
            columns_to_export.insert(0, 'ID')

        event_list = df[columns_to_export].to_dict('records')

        return {
            'event_list': event_list,
            'statistics': {
                'total': len(df),
                'by_severity': df['ä¸¥é‡ç¨‹åº¦'].value_counts().to_dict(),
                'by_platform': df['æ¥æº_æ ‡å‡†'].value_counts().to_dict(),
                'by_status': df['çŠ¶æ€'].value_counts().to_dict()
            }
        }

    def _auto_detect_period(self, df: pd.DataFrame) -> str:
        """è‡ªåŠ¨æ£€æµ‹æŠ¥å‘Šå‘¨æœŸ"""
        if len(df) == 0:
            return datetime.now().strftime('%YQ%q')

        dates = pd.to_datetime(df['åˆ›å»ºæ—¶é—´'], errors='coerce')
        min_date = dates.min()
        max_date = dates.max()

        if pd.isna(min_date) or pd.isna(max_date):
            return datetime.now().strftime('%YQ%q')

        # åˆ¤æ–­æ˜¯å­£åº¦ã€æœˆåº¦è¿˜æ˜¯ä¸“é¡¹æŠ¥å‘Š
        days_diff = (max_date - min_date).days

        if days_diff <= 7:
            return "ä¸“é¡¹æŠ¥å‘Š"
        elif days_diff <= 31:
            return min_date.strftime('%Yå¹´%mæœˆ')
        elif days_diff <= 120:
            quarter = (min_date.month - 1) // 3 + 1
            return f"{min_date.strftime('%Y')}Q{quarter}"
        else:
            return f"{min_date.strftime('%Y')}å¹´åº¦"

    def generate_markdown_report(self, report_data: Dict[str, Any], output_path: str):
        """ç”ŸæˆMarkdownæ ¼å¼æŠ¥å‘Š"""
        md_content = self._render_markdown_template(report_data)

        with open(output_path, 'w', encoding='utf-8') as f:
            f.write(md_content)

    def _render_markdown_template(self, data: Dict[str, Any]) -> str:
        """æ¸²æŸ“Markdownæ¨¡æ¿"""
        summary = data['summary']
        overview = data['overview']
        dist = data['distribution']
        events = data['key_events']
        sentiment = data['sentiment']
        risk = data['risk_assessment']
        recs = data['recommendations']

        md = f"""# {data['hospital_name']}è´Ÿé¢èˆ†æƒ…åˆ†ææŠ¥å‘Š

**æŠ¥å‘Šå‘¨æœŸï¼š** {data['report_period']}
**æŠ¥å‘Šæ—¶é—´ï¼š** {data['generated_time']}
**æŠ¥å‘Šç±»å‹ï¼š** {data['report_type']}

---

## ä¸€ã€æŠ¥å‘Šæ¦‚è¿°

### 1.1 èˆ†æƒ…æ€»ä½“æ€åŠ¿

**{risk['risk_level']}ï¼**

æœ¬å‘¨æœŸå†…å‘ç°è´Ÿé¢èˆ†æƒ…{summary['total_events']}æ¡ï¼Œå…¶ä¸­é«˜é£é™©äº‹ä»¶{summary['high_risk_events']}æ¡ã€‚
ä¼°ç®—å½±å“äººæ•°ï¼š{summary['estimated_reach']}ã€‚
ä¼ æ’­å³°å€¼æ—¶é—´ï¼š{summary['peak_time']}ã€‚
è¶‹åŠ¿ï¼š{summary['trend']}ã€‚

### 1.2 å…³é”®æ•°æ®æ‘˜è¦

| æŒ‡æ ‡ | æ•°å€¼ | è¯´æ˜ |
|------|------|------|
| **è´Ÿé¢èˆ†æƒ…æ€»æ•°** | {summary['total_events']}æ¡ | é«˜é£é™©{summary['high_risk_events']}æ¡ |
| **é«˜å±äº‹ä»¶æ•°é‡** | {summary['high_risk_events']}èµ· | éœ€ç«‹å³å¤„ç† |
| **å½±å“äººæ•°ä¼°ç®—** | {summary['estimated_reach']} | å¹³å°æ’­æ”¾é‡ä¼°ç®— |
| **ä¼ æ’­å³°å€¼æ—¶é—´** | {summary['peak_time']} | é›†ä¸­çˆ†å‘æœŸ |
| **å¹³å‡é£é™©åˆ†** | {summary['average_risk_score']}åˆ† | æ»¡åˆ†100åˆ† |
| **æœªå¤„ç†äº‹ä»¶** | {summary['active_events']}æ¡ | çŠ¶æ€ä¸ºactive |

---

## äºŒã€èˆ†æƒ…åˆ†å¸ƒåˆ†æ

### 2.1 å¹³å°åˆ†å¸ƒ

"""

        # å¹³å°åˆ†å¸ƒè¡¨æ ¼
        for platform, info in dist['platform_distribution']['distribution'].items():
            md += f"- **{platform}**: {info['count']}æ¡ ({info['percentage']}%)\n"

        md += f"""
### 2.2 ç±»å‹åˆ†å¸ƒ

"""

        # ç±»å‹åˆ†å¸ƒ
        for event_type, info in dist['type_distribution']['distribution'].items():
            md += f"- **{event_type}**: {info['count']}æ¡ ({info['percentage']}%)\n"

        md += f"""
### 2.3 ç§‘å®¤åˆ†å¸ƒ

é«˜é£é™©ç§‘å®¤ï¼š
"""

        # ç§‘å®¤åˆ†å¸ƒ
        for dept, count in dist['department_distribution']['high_risk_departments']:
            md += f"- **{dept}**: {count}æ¡\n"

        md += f"""

---

## ä¸‰ã€é‡ç‚¹è´Ÿé¢äº‹ä»¶

"""

        # é‡ç‚¹äº‹ä»¶
        for i, event in enumerate(events, 1):
            reason = (event.get('reason') or '').strip()
            content = (event.get('content') or '').strip()
            if len(reason) > self.event_reason_limit:
                reason = reason[:self.event_reason_limit] + "..."
            if len(content) > self.event_content_limit:
                content = content[:self.event_content_limit] + "..."
            md += f"""
### {i}. {event['title']}

| é¡¹ç›® | è¯¦æƒ… |
|------|------|
| **æ—¶é—´** | {event['time']} |
| **å¹³å°** | {event['platform']} |
| **ç±»å‹** | {event['event_type']} |
| **ç§‘å®¤** | {event['department']} |
| **é£é™©åˆ†** | {event['risk_score']}/100 |
| **çŠ¶æ€** | {event['status']} |

**äº‹ä»¶æ¦‚è¿°ï¼š**
{reason or 'ï¼ˆæš‚æ— ï¼‰'}

**è¯¦ç»†å†…å®¹ï¼š**
{content or 'ï¼ˆæš‚æ— ï¼‰'}

**åŸæ–‡é“¾æ¥ï¼š**
{event.get('link') or 'ï¼ˆæš‚æ— ï¼‰'}

"""

        md += f"""

---

## å››ã€æƒ…æ„Ÿåˆ†æ

### 4.1 æƒ…æ„Ÿå€¾å‘

"""

        # æƒ…æ„Ÿåˆ†å¸ƒ
        for emotion, sentiment_data in sentiment['sentiment_distribution'].items():
            md += f"- **{emotion}**: {sentiment_data['count']}æ¡ ({sentiment_data['percentage']}%)\n"

        md += f"""
**ä¸»è¦æƒ…ç»ªï¼š** {sentiment['dominant_emotion']}
**å¼ºåº¦ï¼š** {sentiment['sentiment_intensity']}

### 4.2 é«˜é¢‘å…³é”®è¯

"""

        # å…³é”®è¯
        for word, count in sentiment['top_keywords'][:15]:
            md += f"- {word} ({count}æ¬¡)\n"

        md += f"""

---

## äº”ã€é£é™©è¯„ä¼°ä¸åº”å¯¹

### 5.1 é£é™©ç­‰çº§

{risk['risk_level']}

### 5.2 ç«‹å³åº”å¯¹æªæ–½ï¼ˆ24å°æ—¶å†…ï¼‰

"""

        # ç«‹å³æªæ–½
        for action in recs['immediate_actions']:
            md += f"{action}\n"

        md += f"""

### 5.3 çŸ­æœŸæªæ–½ï¼ˆ1å‘¨å†…ï¼‰

"""

        # çŸ­æœŸæªæ–½
        for action in recs['short_term_actions']:
            md += f"- {action}\n"

        md += f"""

### 5.4 é•¿æœŸæªæ–½ï¼ˆ1ä¸ªæœˆä»¥ä¸Šï¼‰

"""

        # é•¿æœŸæªæ–½
        for action in recs['long_term_actions']:
            md += f"- {action}\n"

        md += f"""

---

## å…­ã€ç›‘æµ‹é‡ç‚¹

"""

        # ç›‘æµ‹é‡ç‚¹
        for item in recs['monitoring_focus']:
            md += f"- {item}\n"

        md += f"""

---

## ä¸ƒã€é™„å½•

### 7.1 äº‹ä»¶æ¸…å•

| æ—¶é—´ | å¹³å° | ç±»å‹ | é£é™©åˆ† | çŠ¶æ€ |
|------|------|------|--------|------|
"""

        # äº‹ä»¶æ¸…å•
        for event in data['appendix']['event_list'][:20]:
            md += f"| {event['åˆ›å»ºæ—¶é—´']} | {event['æ¥æº']} | {event['ä¸¥é‡ç¨‹åº¦']} | {event['é£é™©åˆ†']} | {event['çŠ¶æ€']} |\n"

        md += f"""

---

**æŠ¥å‘Šç”Ÿæˆæ—¶é—´ï¼š** {data['generated_time']}
**æŠ¥å‘Šæœ‰æ•ˆæœŸï¼š** å»ºè®®æ¯æ—¥æ›´æ–°

---

*æœ¬æŠ¥å‘ŠåŸºäºæä¾›çš„æ•°æ®ç”Ÿæˆï¼Œéƒ¨åˆ†ä¿¡æ¯éœ€æ ¸å®åä½¿ç”¨ã€‚*
"""

        return md

    def generate_word_report(self, report_data: Dict[str, Any], output_path: str):
        """ç”ŸæˆWordæ ¼å¼æŠ¥å‘Š"""
        if not DOCX_AVAILABLE:
            raise ImportError("éœ€è¦å®‰è£…python-docx: pip install python-docx")

        doc = Document()
        summary = report_data['summary']
        dist = report_data['distribution']
        sentiment = report_data['sentiment']
        risk = report_data['risk_assessment']
        recs = report_data['recommendations']
        appendix = report_data['appendix']

        # æ ‡é¢˜
        title = doc.add_heading(f"{report_data['hospital_name']}è´Ÿé¢èˆ†æƒ…åˆ†ææŠ¥å‘Š", 0)

        # æŠ¥å‘Šä¿¡æ¯
        info = doc.add_paragraph()
        info.add_run(f"æŠ¥å‘Šå‘¨æœŸï¼š{report_data['report_period']}\n")
        info.add_run(f"æŠ¥å‘Šæ—¶é—´ï¼š{report_data['generated_time']}\n")
        info.add_run(f"æŠ¥å‘Šç±»å‹ï¼š{report_data['report_type']}")

        # æ¦‚è¿°
        doc.add_heading('ä¸€ã€æŠ¥å‘Šæ¦‚è¿°', 1)
        doc.add_paragraph(f"é£é™©ç­‰çº§ï¼š{risk['risk_level']}")
        doc.add_paragraph(f"è´Ÿé¢èˆ†æƒ…æ€»æ•°ï¼š{summary['total_events']}æ¡")
        doc.add_paragraph(f"é«˜é£é™©äº‹ä»¶ï¼š{summary['high_risk_events']}æ¡")
        doc.add_paragraph(f"å½±å“äººæ•°ä¼°ç®—ï¼š{summary['estimated_reach']}")
        doc.add_paragraph(f"ä¼ æ’­å³°å€¼æ—¶é—´ï¼š{summary['peak_time']}")
        doc.add_paragraph(f"è¶‹åŠ¿åˆ¤æ–­ï¼š{summary['trend']}")

        # é‡ç‚¹äº‹ä»¶
        doc.add_heading('äºŒã€é‡ç‚¹äº‹ä»¶', 1)
        for event in report_data['key_events']:
            doc.add_heading(event['title'], 2)
            p = doc.add_paragraph()
            p.add_run(f"æ—¶é—´ï¼š{event['time']}\n")
            p.add_run(f"å¹³å°ï¼š{event['platform']}\n")
            p.add_run(f"é£é™©åˆ†ï¼š{event['risk_score']}/100\n")
            if event.get('reason'):
                reason = event['reason'][:self.event_reason_limit]
                if len(event['reason']) > self.event_reason_limit:
                    reason += "..."
                p.add_run(f"æ¦‚è¿°ï¼š{reason}\n")
            if event.get('content'):
                content = event['content'][:self.event_content_limit]
                if len(event['content']) > self.event_content_limit:
                    content += "..."
                p.add_run(f"å†…å®¹ï¼š{content}\n")
            if event.get('link'):
                p.add_run(f"åŸæ–‡é“¾æ¥ï¼š{event['link']}\n")

        # èˆ†æƒ…åˆ†å¸ƒåˆ†æ
        doc.add_heading('ä¸‰ã€èˆ†æƒ…åˆ†å¸ƒåˆ†æ', 1)
        doc.add_heading('3.1 å¹³å°åˆ†å¸ƒ', 2)
        for platform, info in dist['platform_distribution']['distribution'].items():
            doc.add_paragraph(f"â€¢ {platform}: {info['count']}æ¡ï¼ˆ{info['percentage']}%ï¼‰")

        doc.add_heading('3.2 ç±»å‹åˆ†å¸ƒ', 2)
        for event_type, info in dist['type_distribution']['distribution'].items():
            doc.add_paragraph(f"â€¢ {event_type}: {info['count']}æ¡ï¼ˆ{info['percentage']}%ï¼‰")

        doc.add_heading('3.3 ç§‘å®¤åˆ†å¸ƒ', 2)
        for dept, count in dist['department_distribution']['high_risk_departments']:
            doc.add_paragraph(f"â€¢ {dept}: {count}æ¡")

        doc.add_heading('3.4 æ—¶é—´åˆ†å¸ƒ', 2)
        time_pattern = dist['time_distribution'].get('time_pattern', 'æœªçŸ¥')
        peak_hours = dist['time_distribution'].get('peak_hours', [])
        if peak_hours:
            peak_desc = "ï¼Œ".join([f"{h}:00({c}æ¡)" for h, c in peak_hours])
        else:
            peak_desc = "æš‚æ— "
        doc.add_paragraph(f"æ—¶é—´è§„å¾‹ï¼š{time_pattern}")
        doc.add_paragraph(f"é«˜å³°æ—¶æ®µï¼š{peak_desc}")

        # æƒ…æ„Ÿåˆ†æ
        doc.add_heading('å››ã€æƒ…æ„Ÿåˆ†æ', 1)
        for emotion, info in sentiment['sentiment_distribution'].items():
            doc.add_paragraph(f"â€¢ {emotion}: {info['count']}æ¡ï¼ˆ{info['percentage']}%ï¼‰")
        doc.add_paragraph(f"ä¸»è¦æƒ…ç»ªï¼š{sentiment['dominant_emotion']}")
        doc.add_paragraph(f"å¼ºåº¦ï¼š{sentiment['sentiment_intensity']}")
        if sentiment.get('top_keywords'):
            keywords_text = "ã€".join([f"{w}({c})" for w, c in sentiment['top_keywords'][:15]])
            doc.add_paragraph(f"é«˜é¢‘å…³é”®è¯ï¼š{keywords_text}")

        # é£é™©è¯„ä¼°
        doc.add_heading('äº”ã€é£é™©è¯„ä¼°', 1)
        doc.add_paragraph(f"æ€»ä½“é£é™©ç­‰çº§ï¼š{risk['risk_level']}")
        doc.add_paragraph(f"é«˜é£é™©äº‹ä»¶æ•°ï¼š{risk['current_risks']['critical']['count']}")
        doc.add_paragraph(f"ä¸­é£é™©äº‹ä»¶æ•°ï¼š{risk['current_risks']['high']['count']}")
        doc.add_paragraph(f"ä½é£é™©äº‹ä»¶æ•°ï¼š{risk['current_risks']['medium']['count']}")
        impact = risk.get('impact_prediction', {})
        if impact:
            doc.add_paragraph(f"çŸ­æœŸå½±å“ï¼š{impact.get('short_term', 'â€”')}")
            doc.add_paragraph(f"ä¸­æœŸå½±å“ï¼š{impact.get('medium_term', 'â€”')}")
            doc.add_paragraph(f"é•¿æœŸå½±å“ï¼š{impact.get('long_term', 'â€”')}")
            doc.add_paragraph(f"æ³•å¾‹é£é™©ï¼š{impact.get('legal_risk', 'â€”')}")

        # åº”å¯¹æªæ–½
        doc.add_heading('å…­ã€åº”å¯¹æªæ–½', 1)
        doc.add_heading('6.1 ç«‹å³åº”å¯¹ï¼ˆ24å°æ—¶å†…ï¼‰', 2)
        for action in recs['immediate_actions']:
            doc.add_paragraph(action)
        doc.add_heading('6.2 çŸ­æœŸæªæ–½ï¼ˆ1å‘¨å†…ï¼‰', 2)
        for action in recs['short_term_actions']:
            doc.add_paragraph(f"â€¢ {action}")
        doc.add_heading('6.3 é•¿æœŸæªæ–½ï¼ˆ1ä¸ªæœˆä»¥ä¸Šï¼‰', 2)
        for action in recs['long_term_actions']:
            doc.add_paragraph(f"â€¢ {action}")

        # ç›‘æµ‹é‡ç‚¹
        doc.add_heading('ä¸ƒã€ç›‘æµ‹é‡ç‚¹', 1)
        for item in recs.get('monitoring_focus', []):
            doc.add_paragraph(f"â€¢ {item}")

        # é™„å½•æ•°æ®
        doc.add_heading('å…«ã€é™„å½•æ•°æ®', 1)
        event_list = appendix.get('event_list', [])[:20]
        if event_list:
            table = doc.add_table(rows=1, cols=5)
            table.style = 'Table Grid'
            hdr = table.rows[0].cells
            hdr[0].text = "æ—¶é—´"
            hdr[1].text = "æ¥æº"
            hdr[2].text = "ä¸¥é‡ç¨‹åº¦"
            hdr[3].text = "é£é™©åˆ†"
            hdr[4].text = "çŠ¶æ€"
            for item in event_list:
                row = table.add_row().cells
                row[0].text = str(item.get('åˆ›å»ºæ—¶é—´', ''))
                row[1].text = str(item.get('æ¥æº', ''))
                row[2].text = str(item.get('ä¸¥é‡ç¨‹åº¦', ''))
                row[3].text = str(item.get('é£é™©åˆ†', ''))
                row[4].text = str(item.get('çŠ¶æ€', ''))
        else:
            doc.add_paragraph("æš‚æ— é™„å½•æ•°æ®")

        # ä¿å­˜
        doc.save(output_path)


if __name__ == "__main__":
    # æµ‹è¯•
    generator = ReportGenerator()
    print("æŠ¥å‘Šç”Ÿæˆå™¨å·²å°±ç»ªï¼")
